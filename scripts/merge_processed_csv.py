import pandas as pd
import os
import re
import shutil 
from datetime import datetime 

# è¨­å®šé …ç›®
APP_ROOT_DIR = r'C:\Users\User26\yoko\dev\csvRead'

PROCESSED_OUTPUT_BASE_DIR = os.path.join(APP_ROOT_DIR, 'processed_output') 
MERGED_OUTPUT_BASE_DIR = os.path.join(APP_ROOT_DIR, 'merged_output') 

FINAL_POSTGRE_COLUMNS = [
    'ocr_result_id', 'page_no', 'id', 'jgroupid_string', 'cif_number', 'settlement_at',
    'maker_name_original', 'maker_name', 'maker_com_code',
    'issue_date_original',      
    'issue_date',
    'due_date_original',        
    'due_date',
    'balance_original',         
    'balance',
    'paying_bank_name_original',
    'paying_bank_name',         
    'paying_bank_branch_name_original', 
    'paying_bank_branch_name',  
    'discount_bank_name_original',
    'discount_bank_name',       
    'description_original',     
    'description'               
]

# ocr_id_mapping ã¨ _ocr_id_sequence_counter: merge_processed_csv_files ã®ãƒ­ãƒ¼ã‚«ãƒ«å¤‰æ•°ã«ç§»å‹•
# ocr_result_id ã®æ™‚åˆ»éƒ¨åˆ†ã‚’å›ºå®šã™ã‚‹ãŸã‚ã®å¤‰æ•°ã‚’ãƒ¡ã‚¤ãƒ³å‡¦ç†é–‹å§‹æ™‚ã«è¨­å®š
_merge_ocr_id_fixed_timestamp_str = "" 

# _generate_expected_ocr_id_for_merge é–¢æ•°ã‚’ä¿®æ­£ã—ã€å›ºå®šæ™‚åˆ»ã‚’ä½¿ç”¨ã™ã‚‹ã‚ˆã†ã«å¤‰æ›´
def _generate_expected_ocr_id_for_merge(group_root_name_local):
    global _merge_ocr_id_fixed_timestamp_str
    # ocr_id_mapping ã¯ã“ã®é–¢æ•°ã‚¹ã‚³ãƒ¼ãƒ—å¤–ã§ç®¡ç†ã•ã‚Œã‚‹ã‹ã€é–¢æ•°å†…ã§ä¸€æ™‚çš„ã«ç”Ÿæˆã•ã‚Œã‚‹
    # process_data.py ã¨åŒã˜ãƒ­ã‚¸ãƒƒã‚¯ã§IDã‚’ã€Œå†è¨ˆç®—ã€ã™ã‚‹
    # process_data.py ã§ã® ocr_id_sequence_counter ã®çŠ¶æ…‹ã‚’çŸ¥ã‚‹ã“ã¨ã¯ã§ããªã„ãŸã‚ã€
    # å„ã‚°ãƒ«ãƒ¼ãƒ—ã®IDãŒæœ¬å½“ã« process_data.py ã¨åŒã˜ã«ãªã‚‹ã‹ç¢ºèªãŒå¿…è¦ã€‚

    # æœ€ã‚‚ç¢ºå®Ÿãªæ–¹æ³•ã¯ã€process_data.py ãŒ ocr_id_mapping ã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜ã—ã€merge_processed_csv.py ãŒãã‚Œã‚’èª­ã¿è¾¼ã‚€ã“ã¨ã€‚
    # ocr_id_mapping ã‚’ _merge_processed_csv_files ã®ä¸­ã§å†ç”Ÿæˆã™ã‚‹ã€‚

    # ã“ã®é–¢æ•°ã¯ã‚‚ã¯ã‚„ã‚°ãƒ­ãƒ¼ãƒãƒ«ãª _ocr_id_sequence_counter ã‚’ä½¿ã‚ãšã€
    # _merge_ocr_id_fixed_timestamp_str ã®ã¿ã‚’ä½¿ã†ã‚ˆã†ã«ã™ã‚‹ã€‚
    # ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ç•ªå·ã®éƒ¨åˆ†ã¯ã€ocr_result_id_mapping_actual ã¨ã„ã†å½¢ã§ã€å®Ÿéš›ã«ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰èª­ã¿å–ã£ãŸIDã‚’ä¿¡é ¼ã™ã‚‹
    pass # ã“ã®é–¢æ•°ã¯å¾Œã§å‰Šé™¤ã‹ã€åˆ¥ã®ä½¿ã„æ–¹ã«å¤‰æ›´

# ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰èª­ã¿è¾¼ã‚“ã  ocr_result_idã‚’ã‚­ãƒ¼ã«ãƒãƒ¼ã‚¸ã™ã‚‹

def merge_processed_csv_files():
    """
    processed_output ãƒ•ã‚©ãƒ«ãƒ€å†…ã®åŠ å·¥æ¸ˆã¿CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã‚°ãƒ«ãƒ¼ãƒ—ã”ã¨ã«çµåˆã—ã€
    merged_output ãƒ•ã‚©ãƒ«ãƒ€ã«ä¿å­˜ã™ã‚‹é–¢æ•°
    """
    global _merge_ocr_id_fixed_timestamp_str # ã‚°ãƒ­ãƒ¼ãƒãƒ«å¤‰æ•°ã‚’ä½¿ç”¨
    
    print(f"--- ãƒ•ã‚¡ã‚¤ãƒ«ã‚°ãƒ«ãƒ¼ãƒ—ã”ã¨ã®çµåˆå‡¦ç†é–‹å§‹ ---")
    print(f"åŠ å·¥æ¸ˆã¿ãƒ•ã‚¡ã‚¤ãƒ«ãƒ•ã‚©ãƒ«ãƒ€: {PROCESSED_OUTPUT_BASE_DIR}")
    print(f"çµåˆæ¸ˆã¿ãƒ•ã‚¡ã‚¤ãƒ«å‡ºåŠ›ãƒ•ã‚©ãƒ«ãƒ€: {MERGED_OUTPUT_BASE_DIR}")

    # çµåˆæ¸ˆã¿ãƒ•ã‚¡ã‚¤ãƒ«å‡ºåŠ›ãƒ•ã‚©ãƒ«ãƒ€ãŒå­˜åœ¨ã—ãªã„å ´åˆã¯ä½œæˆ
    os.makedirs(MERGED_OUTPUT_BASE_DIR, exist_ok=True)

    files_to_merge_by_group = {}
    
    for root, dirs, files in os.walk(PROCESSED_OUTPUT_BASE_DIR): 
        for filename in files:
            if filename.lower().endswith('_processed.csv'):
                match = re.match(r'^(B\d{6})_(\d+)\.jpg_020_processed\.csv$', filename, re.IGNORECASE)
                if match:
                    group_root_name = match.group(1) 
                    page_num = int(match.group(2))   
                    filepath = os.path.join(root, filename)

                    if group_root_name not in files_to_merge_by_group:
                        files_to_merge_by_group[group_root_name] = []
                    files_to_merge_by_group[group_root_name].append((page_num, filepath))
                else:
                    print(f"  â„¹ï¸ ãƒãƒ¼ã‚¸å¯¾è±¡å¤–ã®ãƒ•ã‚¡ã‚¤ãƒ«å½¢å¼ (ãƒ‘ã‚¿ãƒ¼ãƒ³ä¸ä¸€è‡´): {filename}")

    merged_files_count = 0
    sorted_merged_groups = sorted(files_to_merge_by_group.keys())

    # ocr_result_id ã®æ™‚åˆ»éƒ¨åˆ†ã‚’ã€å®Ÿéš›ã«èª­ã¿è¾¼ã‚“ã ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰å–å¾—ã™ã‚‹
    # å„ã‚°ãƒ«ãƒ¼ãƒ—ã®æœ€åˆã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚“ã§ã€ocr_result_id ã® yyyymmddhhmm éƒ¨åˆ†ã‚’å–å¾—ã™ã‚‹
    group_actual_ocr_ids = {}

    for group_root_name in sorted_merged_groups:
        page_files = files_to_merge_by_group[group_root_name]
        page_files.sort(key=lambda x: x[0]) # ãƒšãƒ¼ã‚¸ç•ªå·ã§ã‚½ãƒ¼ãƒˆ
        
        if page_files: # å°‘ãªãã¨ã‚‚1ã¤ã®ãƒ•ã‚¡ã‚¤ãƒ«ãŒã‚ã‚‹å ´åˆ
            first_filepath_in_group = page_files[0][1] # æœ€åˆã®ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹
            try:
                df_first_page = pd.read_csv(first_filepath_in_group, encoding='utf-8-sig', dtype=str, nrows=1) # 1è¡Œã ã‘èª­ã¿è¾¼ã‚€
                if not df_first_page.empty and 'ocr_result_id' in df_first_page.columns:
                    group_actual_ocr_ids[group_root_name] = df_first_page.iloc[0]['ocr_result_id']
                else:
                    print(f"  âš ï¸ è­¦å‘Š: ã‚°ãƒ«ãƒ¼ãƒ— '{group_root_name}' ã®æœ€åˆã®ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ocr_result_idã‚’èª­ã¿è¾¼ã‚ã¾ã›ã‚“ã§ã—ãŸã€‚ã“ã®ã‚°ãƒ«ãƒ¼ãƒ—ã¯çµåˆã•ã‚Œã¾ã›ã‚“ã€‚")
                    files_to_merge_by_group[group_root_name] = [] # çµåˆå¯¾è±¡ã‹ã‚‰é™¤å¤–
            except Exception as e:
                print(f"  âŒ ã‚¨ãƒ©ãƒ¼: ã‚°ãƒ«ãƒ¼ãƒ— '{group_root_name}' ã®æœ€åˆã®ãƒ•ã‚¡ã‚¤ãƒ« ({os.path.basename(first_filepath_in_group)}) èª­ã¿è¾¼ã¿ä¸­ã«ã‚¨ãƒ©ãƒ¼ã€‚ã‚¨ãƒ©ãƒ¼: {e}")
                files_to_merge_by_group[group_root_name] = [] # çµåˆå¯¾è±¡ã‹ã‚‰é™¤å¤–


    for group_root_name in sorted_merged_groups: 
        page_files = files_to_merge_by_group[group_root_name]

        if not page_files: # æœ€åˆã®ãƒ•ã‚¡ã‚¤ãƒ«ã§ã‚¨ãƒ©ãƒ¼ãŒã‚ã£ãŸãªã©ã§çµåˆå¯¾è±¡ã‹ã‚‰é™¤å¤–ã•ã‚ŒãŸå ´åˆ
            continue 
        
        expected_ocr_id_for_group = group_actual_ocr_ids.get(group_root_name)
        if not expected_ocr_id_for_group: # IDãŒå–å¾—ã§ããªã‹ã£ãŸã‚°ãƒ«ãƒ¼ãƒ—ã¯ã‚¹ã‚­ãƒƒãƒ—
            continue
        
        # Bã‚’é™¤ã„ãŸ6æ¡
        expected_cif_number_for_group = group_root_name[1:]
        expected_jgroupid_string_for_group = '001'
        
        combined_df = pd.DataFrame(columns=FINAL_POSTGRE_COLUMNS) 
        
        print(f"  â†’ ã‚°ãƒ«ãƒ¼ãƒ— '{group_root_name}' ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’çµåˆä¸­ (æœŸå¾…ID: {expected_ocr_id_for_group})...")
        
        global_id_counter = 1 

        for page_index, (page_num, filepath) in enumerate(page_files):
            try:
                df_page = pd.read_csv(filepath, encoding='utf-8-sig', dtype=str)
                
                # ãƒ•ã‚¡ã‚¤ãƒ«ãŒç©ºã®å ´åˆ
                if df_page.empty: 
                    print(f"    â„¹ï¸ {os.path.basename(filepath)} ã¯ç©ºã®ãŸã‚ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
                    continue

                # èª­ã¿è¾¼ã‚“ã  df_page ã® ID ãƒ‡ãƒ¼ã‚¿ã‚’å³å¯†ã«ãƒã‚§ãƒƒã‚¯ã™ã‚‹ãƒ­ã‚¸ãƒƒã‚¯ã‚’å¼·åŒ–
                current_file_ocr_id = df_page.iloc[0]['ocr_result_id'] if 'ocr_result_id' in df_page.columns else None
                current_file_jgroupid = df_page.iloc[0]['jgroupid_string'] if 'jgroupid_string' in df_page.columns else None
                current_file_cif = df_page.iloc[0]['cif_number'] if 'cif_number' in df_page.columns else None

                # ocr_result_id ã®ãƒã‚§ãƒƒã‚¯
                if current_file_ocr_id != expected_ocr_id_for_group:
                    print(f"  âŒ ã‚¨ãƒ©ãƒ¼: ã‚°ãƒ«ãƒ¼ãƒ— '{group_root_name}' ã®ãƒ•ã‚¡ã‚¤ãƒ« '{os.path.basename(filepath)}' ã§ocr_result_idã®ä¸ä¸€è‡´ã‚’æ¤œå‡ºã—ã¾ã—ãŸã€‚æœŸå¾…: {expected_ocr_id_for_group}, å®Ÿéš›: {current_file_ocr_id}ã€‚ã“ã®ãƒ•ã‚¡ã‚¤ãƒ«ã¯çµåˆã—ã¾ã›ã‚“ã€‚")
                    continue 

                # cif_number ã®ãƒã‚§ãƒƒã‚¯
                if current_file_cif != expected_cif_number_for_group:
                    print(f"  âŒ ã‚¨ãƒ©ãƒ¼: ã‚°ãƒ«ãƒ¼ãƒ— '{group_root_name}' ã®ãƒ•ã‚¡ã‚¤ãƒ« '{os.path.basename(filepath)}' ã§cif_numberã®ä¸ä¸€è‡´ã‚’æ¤œå‡ºã—ã¾ã—ãŸã€‚æœŸå¾…: {expected_cif_number_for_group}, å®Ÿéš›: {current_file_cif}ã€‚ã“ã®ãƒ•ã‚¡ã‚¤ãƒ«ã¯çµåˆã—ã¾ã›ã‚“ã€‚")
                    continue 

                # jgroupid_string ã®ãƒã‚§ãƒƒã‚¯ã¨ä¿®æ­£
                if current_file_jgroupid != expected_jgroupid_string_for_group:
                    print(f"  âš ï¸ è­¦å‘Š: ã‚°ãƒ«ãƒ¼ãƒ— '{group_root_name}' ã®ãƒ•ã‚¡ã‚¤ãƒ« '{os.path.basename(filepath)}' ã§jgroupid_stringã®ä¸ä¸€è‡´ã‚’æ¤œå‡ºã—ã¾ã—ãŸã€‚æœŸå¾…: {expected_jgroupid_string_for_group}, å®Ÿéš›: {current_file_jgroupid}ã€‚çµåˆå¾Œã€å¼·åˆ¶çš„ã«'{expected_jgroupid_string_for_group}'ã«ä¿®æ­£ã—ã¾ã™ã€‚")
                    df_page['jgroupid_string'] = expected_jgroupid_string_for_group # å¼·åˆ¶ä¿®æ­£

                # çµåˆå‰ã«ã‚«ãƒ©ãƒ é †ã‚’FINAL_POSTGRE_COLUMNSã«åˆã‚ã›ã‚‹
                df_page = df_page[FINAL_POSTGRE_COLUMNS] 

                # df_page: 'id' ã¯ãƒãƒ¼ã‚¸ã•ã‚Œã‚‹å„ãƒ•ã‚¡ã‚¤ãƒ«å†…ã§1ã‹ã‚‰ã®é€£ç•ªã«æŒ¯ã‚Šç›´ã™
                df_page['id'] = range(global_id_counter, global_id_counter + len(df_page))
                global_id_counter += len(df_page) 

                # page_no: å…ƒã®å€¤ã‚’ç¶­æŒã™ã‚‹
                df_page['page_no'] = 1 
                
                combined_df = pd.concat([combined_df, df_page], ignore_index=True)
                print(f"    - ãƒšãƒ¼ã‚¸ {page_num} ({os.path.basename(filepath)}) ã‚’çµåˆã—ã¾ã—ãŸã€‚")
            except Exception as e:
                print(f"  âŒ ã‚¨ãƒ©ãƒ¼: ãƒšãƒ¼ã‚¸ {page_num} ã®ãƒ•ã‚¡ã‚¤ãƒ« {os.path.basename(filepath)} ã®èª­ã¿è¾¼ã¿/çµåˆä¸­ã«å•é¡ŒãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚ã‚¨ãƒ©ãƒ¼: {e}")
                import traceback 
                traceback.print_exc() # ãƒ‡ãƒãƒƒã‚°ã®ãŸã‚è©³ç´°ãªãƒˆãƒ¬ãƒ¼ã‚¹ãƒãƒƒã‚¯ã‚’å‡ºåŠ›
                # ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ãŸå ´åˆã§ã‚‚ã€ä»–ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’çµåˆã—ç¶šã‘ã‚‹ãŸã‚ã«ç©ºã®DataFrameã‚’è¿½åŠ 
                combined_df = pd.concat([combined_df, pd.DataFrame(columns=FINAL_POSTGRE_COLUMNS)], ignore_index=True)


        # çµåˆã•ã‚ŒãŸDataFrameã‚’merged_output_filenameã«ä¿å­˜
        merged_output_filename = f"{group_root_name}_processed_merged.csv"
        merged_output_filepath = os.path.join(MERGED_OUTPUT_BASE_DIR, merged_output_filename)
        
        try:
            if not combined_df.empty: 
                combined_df.to_csv(merged_output_filepath, index=False, encoding='utf-8-sig')
                merged_files_count += 1
                print(f"  âœ… ã‚°ãƒ«ãƒ¼ãƒ— '{group_root_name}' ã®çµåˆãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¿å­˜ã—ã¾ã—ãŸ: {merged_output_filepath}")
            else:
                print(f"  âš ï¸ è­¦å‘Š: ã‚°ãƒ«ãƒ¼ãƒ— '{group_root_name}' ã«çµåˆå¯¾è±¡ã®æœ‰åŠ¹ãªãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚‰ãªã‹ã£ãŸãŸã‚ã€ãƒ•ã‚¡ã‚¤ãƒ«ã¯ä¿å­˜ã•ã‚Œã¾ã›ã‚“ã€‚")
        except Exception as e:
            print(f"  âŒ ã‚¨ãƒ©ãƒ¼: ã‚°ãƒ«ãƒ¼ãƒ— '{group_root_name}' ã®çµåˆãƒ•ã‚¡ã‚¤ãƒ«ã®ä¿å­˜ä¸­ã«å•é¡ŒãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚ã‚¨ãƒ©ãƒ¼: {e}")

    print(f"\n--- ãƒ•ã‚¡ã‚¤ãƒ«ã‚°ãƒ«ãƒ¼ãƒ—ã”ã¨ã®çµåˆå‡¦ç†å®Œäº† ---")
    print(f"ğŸ‰ çµåˆã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ã‚°ãƒ«ãƒ¼ãƒ—æ•°: {merged_files_count} ğŸ‰")

if __name__ == "__main__":
    print(f"--- çµåˆå‡¦ç†ã‚¹ã‚¯ãƒªãƒ—ãƒˆé–‹å§‹: {datetime.now()} ---")
    merge_processed_csv_files()
    print(f"\nğŸ‰ å…¨ã¦ã®çµåˆå‡¦ç†ãŒå®Œäº†ã—ã¾ã—ãŸï¼ ({datetime.now()}) ğŸ‰")
    