import pandas as pd
import os
import re
from datetime import datetime
import random 
import shutil
import numpy as np 

INPUT_BASE_DIR = r'G:\å…±æœ‰ãƒ‰ãƒ©ã‚¤ãƒ–\VLM-OCR\20_æ•™å¸«ãƒ‡ãƒ¼ã‚¿\30_output_csv' 
APP_ROOT_DIR = r'C:\Users\User26\yoko\dev\csvRead'
SEARCH_RESULT_OUTPUT_BASE_DIR = os.path.join(APP_ROOT_DIR, 'filtered_originals')
PROCESSED_OUTPUT_BASE_DIR = os.path.join(APP_ROOT_DIR, 'processed_output') 
MASTER_DATA_DIR = os.path.join(APP_ROOT_DIR, 'master_data')

# å…¨21ã‚«ãƒ©ãƒ 
FINAL_POSTGRE_COLUMNS = [
    'ocr_result_id', 'page_no', 'id', 'jgroupid_string', 'cif_number', 'settlement_at',
    'maker_name_original', 'maker_name', 'maker_com_code',
    'issue_date_rightside_date', 'issue_date',
    'due_date_rightside_date', 'due_date',
    'balance_rightside', 'balance',
    'payment_bank_name_rightside', 
    'payment_bank_name',           
    'payment_bank_branch_name_rightside', 
    'payment_bank_branch_name',    
    'description_rightside',       
    'description'                  
]


# --- å„CSVãƒ•ã‚¡ã‚¤ãƒ«å½¢å¼ã”ã¨ã®ãƒãƒƒãƒ”ãƒ³ã‚°ãƒ«ãƒ¼ãƒ«ã‚’å®šç¾© ---
HAND_BILL_MAPPING_DICT = {
    'maker_name': 'æŒ¯å‡ºäºº',
    'issue_date': 'æŒ¯å‡ºå¹´æœˆæ—¥',
    'due_date': 'æ”¯æ‰•æœŸæ—¥',
    'balance': 'é‡‘é¡', 
    'payment_bank_name': 'æ”¯æ‰•éŠ€è¡Œåç§°',            
    'payment_bank_branch_name': 'æ”¯æ‰•éŠ€è¡Œæ”¯åº—å',   
    'description_rightside': 'å‰²å¼•éŠ€è¡ŒååŠã³æ”¯åº—åç­‰', # 'å‰²å¼•éŠ€è¡ŒååŠã³æ”¯åº—åç­‰' ã‚’ 'description_rightside' ã¸
    'description': 'æ‘˜è¦'                       # 'æ‘˜è¦' ã‚’ 'description' ã¸
}

FINANCIAL_STATEMENT_MAPPING_DICT = {
    'maker_name': 'account', 
    'issue_date': 'amount_0', 
    'balance': 'amount_0',    
    'due_date': 'amount_1',   
    'description': 'amount_2' 
}

LOAN_DETAILS_MAPPING_DICT = {
    'maker_name': 'å€Ÿå…¥å…ˆåç§°(æ°å)',
    'issue_date': 'å€Ÿå…¥å…ˆæ‰€åœ¨åœ°(ä½æ‰€)', 
    'balance': 'æœŸæœ«ç¾åœ¨é«˜',           
    'description_rightside': 'æœŸä¸­ã®æ”¯æ‰•åˆ©å­é¡', 
    'description': 'åˆ©ç‡',            
}

NO_HEADER_MAPPING_DICT = {
    'maker_name': 0, 
    'issue_date': 1, 
    'due_date': 2,   
    'payment_bank_name': 3, 
    'payment_bank_branch_name': 4, 
    'balance': 5,    
    'description_rightside': 6, 
    'description': 7, 
}


# --- é–¢æ•°å®šç¾© ---
current_ocr_id_sequence = 0 
maker_name_to_com_code_map = {}
next_maker_com_code_val = 100 
current_jgroupid_index = 0 
jgroupid_values_from_master = [] 


def get_next_ocr_id():
    global current_ocr_id_sequence 
    current_ocr_id_sequence += 1
    return str(current_ocr_id_sequence % 10000).zfill(4)

def get_next_jgroupid_string():
    global current_jgroupid_index 
    global jgroupid_values_from_master 

    if jgroupid_values_from_master:
        jgroupid_val = jgroupid_values_from_master[current_jgroupid_index % len(jgroupid_values_from_master)]
        current_jgroupid_index += 1
        return str(jgroupid_val).zfill(3) 
    else:
        return "000" 

def get_maker_com_code_for_name(maker_name):
    global maker_name_to_com_code_map 
    global next_maker_com_code_val 

    maker_name_str = str(maker_name).strip() 
    
    if not maker_name_str: 
        return "" 

    if maker_name_str in maker_name_to_com_code_map:
        return maker_name_to_com_code_map[maker_name_str]
    else:
        new_code_int = next_maker_com_code_val % 1000 
        if new_code_int < 100: 
            new_code_int = 100 + new_code_int 
        new_code = str(new_code_int).zfill(3) 
        
        maker_name_to_com_code_map[maker_name_str] = new_code
        next_maker_com_code_val += 1
        return new_code


def process_universal_csv(input_filepath, processed_output_base_dir, input_base_dir, 
                        maker_master_df, 
                        final_postgre_columns_list, no_header_map, hand_bill_map, financial_map, loan_map):
    """
    å…¨ã¦ã®AIReadå‡ºåŠ›CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿ã€çµ±ä¸€ã•ã‚ŒãŸPostgreSQLå‘ã‘ã‚«ãƒ©ãƒ å½¢å¼ã«å¤‰æ›ã—ã¦å‡ºåŠ›
    CSVã®ç¨®é¡ï¼ˆãƒ˜ãƒƒãƒ€ãƒ¼å†…å®¹ï¼‰ã‚’åˆ¤åˆ¥ã—ã€ãã‚Œãã‚Œã«å¿œã˜ãŸãƒãƒƒãƒ”ãƒ³ã‚°ã‚’é©ç”¨
    """
    df_original = None
    file_type = "ä¸æ˜" 
    
    try:
        encodings_to_try = ['utf-8', 'utf-8-sig', 'shift_jis', 'cp932']
        
        for enc in encodings_to_try:
            try:
                # keep_default_na=False ã§ç©ºæ–‡å­—åˆ—ã¯ NaN ã«å¤‰æ›ã—ãªã„
                # na_values=['ã€ƒ'] ã§ 'ã€ƒ' ã®ã¿ NaN ã«ã™ã‚‹
                df_original = pd.read_csv(input_filepath, encoding=enc, header=0, sep=',', quotechar='"', 
                                        dtype=str, na_values=['ã€ƒ'], keep_default_na=False)
                
                # èª­ã¿è¾¼ã‚“ã ã‚«ãƒ©ãƒ åã‚’ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ï¼ˆå‰å¾Œã®ç©ºç™½é™¤å»ï¼‰
                df_original.columns = df_original.columns.str.strip() 
                
                current_headers = df_original.columns.tolist()

                is_hand_bill = ('æŒ¯å‡ºäºº' in current_headers) and ('é‡‘é¡' in current_headers)
                is_financial = ('account' in current_headers)
                is_loan = ('å€Ÿå…¥å…ˆåç§°(æ°å)' in current_headers)

                if is_hand_bill:
                    file_type = "æ‰‹å½¢æƒ…å ±"
                elif is_financial:
                    file_type = "è²¡å‹™è«¸è¡¨"
                elif is_loan:
                    file_type = "å€Ÿå…¥é‡‘æ˜ç´°"
                else:
                    file_type = "æ±ç”¨ãƒ‡ãƒ¼ã‚¿_ãƒ˜ãƒƒãƒ€ãƒ¼ãªã—"
                    # ãƒ˜ãƒƒãƒ€ãƒ¼ãªã—ã®å ´åˆã‚‚åŒã˜èª­ã¿è¾¼ã¿ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã‚’é©ç”¨
                    df_original = pd.read_csv(input_filepath, encoding=enc, header=None, sep=',', quotechar='"', 
                                            dtype=str, na_values=['ã€ƒ'], keep_default_na=False)
                    df_original.columns = df_original.columns.astype(str).str.strip() 
                
                print(f"  ãƒ‡ãƒãƒƒã‚°: ãƒ•ã‚¡ã‚¤ãƒ« {os.path.basename(input_filepath)} ã®åˆ¤å®šçµæœ: '{file_type}'")
                print(f"  ãƒ‡ãƒãƒƒã‚°: èª­ã¿è¾¼ã‚“ã  df_original ã®ã‚«ãƒ©ãƒ :\n{df_original.columns.tolist()}")
                print(f"  ãƒ‡ãƒãƒƒã‚°: èª­ã¿è¾¼ã‚“ã  df_original ã®æœ€åˆã®3è¡Œ:\n{df_original.head(3).to_string()}") 
                print(f"  ãƒ‡ãƒãƒƒã‚°: df_originalå†…ã®æ¬ æå€¤ (NaN) ã®æ•°:\n{df_original.isnull().sum().to_string()}") 
                    
                break 
            except Exception as e_inner: 
                print(f"  ãƒ•ã‚¡ã‚¤ãƒ« {os.path.basename(input_filepath)} ã‚’ {enc} ã§èª­ã¿è¾¼ã¿å¤±æ•—ã€‚åˆ¥ã®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚’è©¦ã—ã¾ã™ã€‚ã‚¨ãƒ©ãƒ¼: {e_inner}")
                df_original = None 
                continue 

        if df_original is None or df_original.empty:
            print(f"  è­¦å‘Š: ãƒ•ã‚¡ã‚¤ãƒ« {os.path.basename(input_filepath)} ã‚’ã©ã®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã¨ãƒ˜ãƒƒãƒ€ãƒ¼è¨­å®šã§ã‚‚èª­ã¿è¾¼ã‚ã¾ã›ã‚“ã§ã—ãŸã€‚å‡¦ç†ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
            return 
        
        print(f"  ãƒ•ã‚¡ã‚¤ãƒ« {os.path.basename(input_filepath)} ã¯ '{file_type}' ã¨ã—ã¦å‡¦ç†ã—ã¾ã™ã€‚")

    except Exception as e:
        print(f"âŒ ã‚¨ãƒ©ãƒ¼ç™ºç”Ÿï¼ˆ{input_filepath}ï¼‰: CSVèª­ã¿è¾¼ã¿ã¾ãŸã¯ãƒ•ã‚¡ã‚¤ãƒ«ã‚¿ã‚¤ãƒ—åˆ¤åˆ¥ã§å•é¡ŒãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚ã‚¨ãƒ©ãƒ¼: {e}")
        import traceback
        traceback.print_exc()
        return

    # --- ãƒ‡ãƒ¼ã‚¿åŠ å·¥å‡¦ç† ---
    df_data_rows = df_original.copy() 

    if df_data_rows.empty:
        print(f"  è­¦å‘Š: ãƒ•ã‚¡ã‚¤ãƒ« {os.path.basename(input_filepath)} ã«æœ‰åŠ¹ãªãƒ‡ãƒ¼ã‚¿è¡ŒãŒè¦‹ã¤ã‹ã‚‰ãªã‹ã£ãŸãŸã‚ã€åŠ å·¥ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
        return 

    # ã€Œã€ƒã€ã®ã¿ã‚’ffillã§åŸ‹ã‚ã€ç©ºæ–‡å­—åˆ—ã¯ãã®ã¾ã¾ç¶­æŒ
    df_data_rows = df_data_rows.ffill() 
    df_data_rows = df_data_rows.fillna('') 
    print(f"  â„¹ï¸ ã€Œã€ƒã€ãƒãƒ¼ã‚¯ã‚’ç›´ä¸Šãƒ‡ãƒ¼ã‚¿ã§åŸ‹ã‚ã€å…ƒã€…ãƒ–ãƒ©ãƒ³ã‚¯ã ã£ãŸç®‡æ‰€ã¯ç¶­æŒã—ã¾ã—ãŸã€‚")

    # åˆè¨ˆè¡Œã®å‰Šé™¤ãƒ­ã‚¸ãƒƒã‚¯
    keywords_to_delete = ["åˆè¨ˆ", "å°è¨ˆ", "è¨ˆ", "æ‰‹æŒæ‰‹å½¢è¨ˆ", "å‰²å¼•æ‰‹å½¢è¨ˆ"] # æ‰‹å½¢è¨ˆã‚‚è¿½åŠ 
    
    filter_conditions = []
    if file_type == "æ‰‹å½¢æƒ…å ±":
        if 'æŒ¯å‡ºäºº' in df_data_rows.columns:
            filter_conditions.append(df_data_rows['æŒ¯å‡ºäºº'].isin(keywords_to_delete))
    elif file_type == "è²¡å‹™è«¸è¡¨":
        if 'account' in df_data_rows.columns:
            filter_conditions.append(df_data_rows['account'].isin(keywords_to_delete))
    elif file_type == "å€Ÿå…¥é‡‘æ˜ç´°":
        if 'å€Ÿå…¥å…ˆåç§°(æ°å)' in df_data_rows.columns:
            filter_conditions.append(df_data_rows['å€Ÿå…¥å…ˆåç§°(æ°å)'].isin(keywords_to_delete))
    elif file_type == "æ±ç”¨ãƒ‡ãƒ¼ã‚¿_ãƒ˜ãƒƒãƒ€ãƒ¼ãªã—":
        if '0' in df_data_rows.columns: 
            filter_conditions.append(df_data_rows['0'].isin(keywords_to_delete))

    if filter_conditions:
        combined_filter = pd.concat(filter_conditions, axis=1).any(axis=1)
        rows_deleted_count = combined_filter.sum()
        df_data_rows = df_data_rows[~combined_filter].reset_index(drop=True)
        if rows_deleted_count > 0:
            print(f"  â„¹ï¸ åˆè¨ˆè¡Œï¼ˆã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰: {', '.join(keywords_to_delete)}ï¼‰ã‚’ {rows_deleted_count} è¡Œå‰Šé™¤ã—ã¾ã—ãŸã€‚")
    
    num_rows_to_process = len(df_data_rows) 
    
    # å¿…è¦ãªã‚«ãƒ©ãƒ åã‚’æŒã¤ç©ºã®DataFrameã‚’ä½œæˆã—ã€å…¨ã‚»ãƒ«ã‚’ç©ºæ–‡å­—åˆ—ã§åˆæœŸåŒ–
    df_processed = pd.DataFrame('', index=range(num_rows_to_process), columns=final_postgre_columns_list)


    # --- å…±é€šé …ç›® (PostgreSQLã®ã‚°ãƒªãƒ¼ãƒ³ã®è¡¨ã®å·¦å´ã«æ¥ã‚‹ã€è‡ªå‹•ç”Ÿæˆé …ç›®) ã‚’ç”Ÿæˆ ---
    ocr_result_id_val = get_next_ocr_id() 
    df_processed['ocr_result_id'] = [ocr_result_id_val] * num_rows_to_process 

    df_processed['page_no'] = [1] * num_rows_to_process 

    df_processed['id'] = range(1, num_rows_to_process + 1)

    jgroupid_string_val = get_next_jgroupid_string() 
    df_processed['jgroupid_string'] = [jgroupid_string_val] * num_rows_to_process

    cif_number_val = str(random.randint(100000, 999999))
    df_processed['cif_number'] = [cif_number_val] * num_rows_to_process

    settlement_at_val = datetime.now().strftime('%Y%m') 
    df_processed['settlement_at'] = [settlement_at_val] * num_rows_to_process

    # --- å„ãƒ•ã‚¡ã‚¤ãƒ«ã‚¿ã‚¤ãƒ—ã«å¿œã˜ãŸãƒãƒƒãƒ”ãƒ³ã‚°ãƒ«ãƒ¼ãƒ«ã‚’é©ç”¨ ---
    mapping_to_use = {}
    if file_type == "æ‰‹å½¢æƒ…å ±":
        mapping_to_use = hand_bill_map
    elif file_type == "è²¡å‹™è«¸è¡¨": 
        mapping_to_use = financial_map
    elif file_type == "å€Ÿå…¥é‡‘æ˜ç´°": 
        mapping_to_use = loan_map
    else: 
        mapping_to_use = no_header_map

    df_data_rows.columns = df_data_rows.columns.astype(str) # å¿µã®ãŸã‚ã“ã“ã§ã‚‚strã«å¤‰æ›
    
    for pg_col_name, src_ref in mapping_to_use.items():
        source_data_series = None
        if isinstance(src_ref, str): 
            if src_ref in df_data_rows.columns: 
                source_data_series = df_data_rows[src_ref]
            else:
                print(f"  âš ï¸ è­¦å‘Š: ãƒãƒƒãƒ”ãƒ³ã‚°å…ƒã®ã‚«ãƒ©ãƒ  '{src_ref}' ãŒå…ƒã®CSVãƒ•ã‚¡ã‚¤ãƒ«ã«è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸï¼ˆPostgreSQLã‚«ãƒ©ãƒ : {pg_col_name}ï¼‰ã€‚ã“ã®ã‚«ãƒ©ãƒ ã¯ãƒ–ãƒ©ãƒ³ã‚¯ã«ãªã‚Šã¾ã™ã€‚")
        elif isinstance(src_ref, int): 
            if str(src_ref) in df_data_rows.columns: 
                source_data_series = df_data_rows[str(src_ref)]
            elif src_ref < df_data_rows.shape[1]: 
                source_data_series = df_data_rows.iloc[:, src_ref]
            else:
                print(f"  âš ï¸ è­¦å‘Š: ãƒãƒƒãƒ”ãƒ³ã‚°å…ƒã®åˆ—ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ '{src_ref}' ãŒå…ƒã®CSVãƒ•ã‚¡ã‚¤ãƒ«ã«å­˜åœ¨ã—ã¾ã›ã‚“ï¼ˆPostgreSQLã‚«ãƒ©ãƒ : {pg_col_name}ï¼‰ã€‚ã“ã®ã‚«ãƒ©ãƒ ã¯ãƒ–ãƒ©ãƒ³ã‚¯ã«ãªã‚Šã¾ã™ã€‚")

        if source_data_series is not None:
            df_processed[pg_col_name] = source_data_series.astype(str).values 
        else:
            pass 


    # --- Excelé–¢æ•°ç›¸å½“ã®ãƒ­ã‚¸ãƒƒã‚¯ã‚’é©ç”¨ï¼ˆæ´¾ç”Ÿã‚«ãƒ©ãƒ ã®ç”Ÿæˆï¼‰ ---
    # â˜…â˜…â˜… å„ã‚«ãƒ©ãƒ ã®ç”Ÿæˆãƒ­ã‚¸ãƒƒã‚¯ã‚’Excelç”»åƒã«å¿ å®Ÿã«å†ç¾ã™ã‚‹ï¼ â˜…â˜…â˜…
    
    df_processed['maker_name_original'] = df_processed['maker_name'].copy() 
    
    df_processed['maker_com_code'] = df_processed['maker_name'].apply(get_maker_com_code_for_name)

    df_processed['issue_date_rightside_date'] = df_processed['issue_date'].copy() 
    df_processed['due_date_rightside_date'] = df_processed['due_date'].copy()   

    def clean_balance_no_comma(value):
        try:
            cleaned_value = str(value).replace(',', '').strip()
            if not cleaned_value:
                return '' 
            numeric_value = float(cleaned_value)
            return str(int(numeric_value)) 
        except ValueError:
            return '' 
    
    df_processed['balance'] = df_processed['balance'].apply(clean_balance_no_comma)
    df_processed['balance_rightside'] = df_processed['balance'].copy() 

    # payment_bank_name_rightside, payment_bank_name, payment_bank_branch_name_rightside, payment_bank_branch_name, description_rightside, description
    # ã“ã‚Œã‚‰ã®ã‚«ãƒ©ãƒ ã¯ FINAL_POSTGRE_COLUMNS ã«ã‚ã‚‹åŸºæœ¬çš„ãªã‚«ãƒ©ãƒ ã§ã€HAND_BILL_MAPPING_DICT ã§å…ƒã®CSVã‹ã‚‰ç›´æ¥ãƒãƒƒãƒ”ãƒ³ã‚°ã•ã‚Œã‚‹
    # ãã‚Œã‚‰ã®å€¤ã‹ã‚‰ã€Excelç”»åƒã«è¦‹ã‚‰ã‚Œã‚‹ã€Œã‚³ãƒ”ãƒ¼ã€é–¢ä¿‚ã‚’å†ç¾
    
    df_processed['payment_bank_name_rightside'] = df_processed['payment_bank_name'].copy() 
    df_processed['payment_bank_branch_name_rightside'] = df_processed['payment_bank_branch_name'].copy() 
    
    # description_rightside ã¯ HAND_BILL_MAPPING_DICT ã§ 'å‰²å¼•éŠ€è¡ŒååŠã³æ”¯åº—åç­‰' ã‹ã‚‰ç›´æ¥ãƒãƒƒãƒ”ãƒ³ã‚°ã•ã‚Œã¦ã„ã‚‹
    # description ã¯ HAND_BILL_MAPPING_DICT ã§ 'æ‘˜è¦' ã‹ã‚‰ç›´æ¥ãƒãƒƒãƒ”ãƒ³ã‚°ã•ã‚Œã¦ã„ã‚‹

    # â˜…â˜…â˜… ä¿®æ­£ã“ã“ã¾ã§ï¼ˆã“ã‚Œä»¥ä¸Šã€FINAL_POSTGRE_COLUMNSã«ãªã„ã‚«ãƒ©ãƒ ã¸ã®æ´¾ç”Ÿãƒ­ã‚¸ãƒƒã‚¯ã¯åŠ ãˆãªã„ï¼‰ â˜…â˜…â˜…
    
    # --- ä¿å­˜å‡¦ç† ---
    relative_path_to_file = os.path.relpath(input_filepath, input_base_dir)
    relative_dir_to_file = os.path.dirname(relative_path_to_file)
    processed_output_sub_dir = os.path.join(processed_output_base_dir, relative_dir_to_file)
    os.makedirs(processed_output_sub_dir, exist_ok=True)

    processed_output_filename = os.path.basename(input_filepath).replace('.csv', '_processed.csv')
    processed_output_filepath = os.path.join(processed_output_sub_dir, processed_output_filename)
    df_processed.to_csv(processed_output_filepath, index=False, encoding='utf-8-sig')

    print(f"âœ… åŠ å·¥å®Œäº†: {input_filepath} -> {processed_output_filepath}")

# --- ãƒ¡ã‚¤ãƒ³å‡¦ç† ---
if __name__ == "__main__":
    print(f"--- å‡¦ç†é–‹å§‹: {datetime.now()} ---")

    os.makedirs(PROCESSED_OUTPUT_BASE_DIR, exist_ok=True) 

    MASTER_DATA_DIR = os.path.join(APP_ROOT_DIR, 'master_data') 

    maker_master_filepath = os.path.join(MASTER_DATA_DIR, 'master.csv') 
    maker_master_df = pd.DataFrame() 
    if os.path.exists(maker_master_filepath):
        try:
            maker_master_df = pd.read_csv(maker_master_filepath, encoding='utf-8')
            print(f"  â„¹ï¸ {maker_master_filepath} ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ (ã“ã®ãƒ‡ãƒ¼ã‚¿ã¯ç¾åœ¨ã®maker_com_codeç”Ÿæˆã«ã¯ä½¿ç”¨ã•ã‚Œã¾ã›ã‚“)ã€‚")
        except Exception as e:
            print(f"âŒ ã‚¨ãƒ©ãƒ¼: {maker_master_filepath} ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸã€‚ã‚¨ãƒ©ãƒ¼: {e}")
            maker_master_df = pd.DataFrame() 
    else:
        print(f"âš ï¸ è­¦å‘Š: {maker_master_filepath} ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ (ç¾åœ¨ã®maker_com_codeç”Ÿæˆã«ã¯å½±éŸ¿ã‚ã‚Šã¾ã›ã‚“)ã€‚")
        maker_master_df = pd.DataFrame() 


    jgroupid_master_filepath = os.path.join(MASTER_DATA_DIR, 'jgroupid_master.csv')
    
    if os.path.exists(jgroupid_master_filepath): 
        try:
            df_jgroupid_temp = pd.read_csv(jgroupid_master_filepath, encoding='utf-8', header=None)
            
            if not df_jgroupid_temp.empty and df_jgroupid_temp.shape[1] > 0:
                jgroupid_values_from_master = df_jgroupid_temp.iloc[:, 0].astype(str).tolist()
                if not jgroupid_values_from_master:
                    raise ValueError("jgroupid_master.csv ã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã‚ã¾ã—ãŸãŒã€ãƒªã‚¹ãƒˆãŒç©ºã§ã™ã€‚")
            else:
                raise ValueError("jgroupid_master.csv ãŒç©ºã¾ãŸã¯ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
            
        except Exception as e:
            print(f"âŒ ã‚¨ãƒ©ãƒ¼: jgroupid_master.csv ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸã€‚ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã¾ãŸã¯ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚ã‚¨ãƒ©ãƒ¼: {e}")
            jgroupid_values_from_master = [str(i).zfill(3) for i in range(1, 94)] 
    else:
        print(f"âš ï¸ è­¦å‘Š: jgroupid_master.csv ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚ãƒ‘ã‚¹ã‚’ç¢ºèªã—ã¦ãã ã•ã„: {jgroupid_master_filepath}")
        jgroupid_values_from_master = [str(i).zfill(3) for i in range(1, 94)] 

    INPUT_PROCESSED_DIR = os.path.join(APP_ROOT_DIR, 'filtered_originals') 

    for root, dirs, files in os.walk(INPUT_PROCESSED_DIR):
        for filename in files:
            if filename.lower().endswith('.csv') and not filename.lower().endswith('_processed.csv'): 
                input_filepath = os.path.join(root, filename)
                print(f"\n--- å‡¦ç†å¯¾è±¡ãƒ•ã‚¡ã‚¤ãƒ«: {input_filepath} ---")

                process_universal_csv(input_filepath, PROCESSED_OUTPUT_BASE_DIR, INPUT_PROCESSED_DIR, 
                                    maker_master_df, 
                                    FINAL_POSTGRE_COLUMNS, NO_HEADER_MAPPING_DICT, HAND_BILL_MAPPING_DICT, 
                                    FINANCIAL_STATEMENT_MAPPING_DICT, LOAN_DETAILS_MAPPING_DICT)

    print(f"\nğŸ‰ å…¨ã¦ã®ãƒ•ã‚¡ã‚¤ãƒ«ã®åŠ å·¥å‡¦ç†ãŒå®Œäº† ({datetime.now()}) ğŸ‰")
    