import pandas as pd
import os
import re
from datetime import datetime
import random 
import shutil
import numpy as np 
import json 

# --- è¨­å®šé …ç›® ---
INPUT_BASE_DIR = r'G:\å…±æœ‰ãƒ‰ãƒ©ã‚¤ãƒ–\å•†å·¥ä¸­é‡‘\202412_å‹˜å®šç§‘ç›®æ˜ç´°æœ¬ç•ªç¨¼åƒ\50_æ¤œè¨¼\010_åå¯¾å‹˜å®šæ€§èƒ½è©•ä¾¡\20_ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿\ä½œæˆãƒ¯ãƒ¼ã‚¯\10_å—å–æ‰‹å½¢\Import' 
APP_ROOT_DIR = r'C:\Users\User26\yoko\dev\csvRead'
SEARCH_RESULT_OUTPUT_BASE_DIR = os.path.join(APP_ROOT_DIR, 'filtered_originals')
PROCESSED_OUTPUT_BASE_DIR = os.path.join(APP_ROOT_DIR, 'processed_output') 
MASTER_DATA_DIR = os.path.join(APP_ROOT_DIR, 'master_data')

FINAL_POSTGRE_COLUMNS = [
    'ocr_result_id',
    'page_no',
    'id',
    'jgroupid_string',
    'cif_number',
    'settlement_at',
    'registration_number_original',
    'registration_number',
    'maker_name_original',
    'maker_name',
    'maker_com_code',
    'maker_com_code_status_id',
    'maker_comcd_relation_source_type_id',
    'maker_exist_comcd_relation_history_id',
    'issue_date_original',
    'issue_date',
    'due_date_original',
    'due_date',
    'paying_bank_name_original',
    'paying_bank_name',
    'paying_bank_code',
    'paying_bank_branch_name_original',
    'paying_bank_branch_name',
    'balance_original',
    'balance',
    'discount_bank_name_original',
    'discount_bank_name',
    'discount_bank_code',
    'description_original',
    'description',
    'conf_registration_number',
    'conf_maker_name',
    'conf_issue_date',
    'conf_due_date',
    'conf_balance',
    'conf_paying_bank_name',
    'conf_paying_bank_branch_name',
    'conf_discount_bank_name',
    'conf_description',
    'coord_x_registration_number',
    'coord_y_registration_number',
    'coord_h_registration_number',
    'coord_w_registration_number',
    'coord_x_maker_name',
    'coord_y_maker_name',
    'coord_h_maker_name',
    'coord_w_maker_name',
    'coord_x_issue_date',
    'coord_y_issue_date',
    'coord_h_issue_date',
    'coord_w_issue_date',
    'coord_x_due_date',
    'coord_y_due_date',
    'coord_h_due_date',
    'coord_w_due_date',
    'coord_x_balance',
    'coord_y_balance',
    'coord_h_balance',
    'coord_w_balance',
    'coord_x_paying_bank_name',
    'coord_y_paying_bank_name',
    'coord_h_paying_bank_name',
    'coord_w_paying_bank_name',
    'coord_x_paying_bank_branch_name',
    'coord_y_paying_bank_branch_name',
    'coord_h_paying_bank_branch_name',
    'coord_w_paying_bank_branch_name',
    'coord_x_discount_bank_name',
    'coord_y_discount_bank_name',
    'coord_h_discount_bank_name',
    'coord_w_discount_bank_name',
    'coord_x_description',
    'coord_y_description',
    'coord_h_description',
    'coord_w_description',
    'row_no',
    'insertdatetime',
    'updatedatetime',
    'updateuser'
]


# --- å„CSVãƒ•ã‚¡ã‚¤ãƒ«å½¢å¼ã”ã¨ã®ãƒãƒƒãƒ”ãƒ³ã‚°ãƒ«ãƒ¼ãƒ«ã‚’å®šç¾© ---
# å—å–æ‰‹å½¢ã‚¢ãƒ—ãƒª (B*020.csv) ã®ãƒãƒƒãƒ”ãƒ³ã‚°
HAND_BILL_MAPPING_DICT = {
    'maker_name': 'æŒ¯å‡ºäºº',
    'issue_date': 'æŒ¯å‡ºå¹´æœˆæ—¥',                    
    'due_date': 'æ”¯æ‰•æœŸæ—¥',                        
    'balance': 'é‡‘é¡',                            
    'paying_bank_name': 'æ”¯æ‰•éŠ€è¡Œåç§°',            
    'paying_bank_branch_name': 'æ”¯æ‰•éŠ€è¡Œæ”¯åº—å',   
    'discount_bank_name': 'å‰²å¼•éŠ€è¡ŒååŠã³æ”¯åº—åç­‰', 
    'description': 'æ‘˜è¦'                       
}

# ãã®ä»–ã®ãƒãƒƒãƒ”ãƒ³ã‚°è¾æ›¸ã¯ã€ã“ã®ã‚¢ãƒ—ãƒªã§ä»–ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚¿ã‚¤ãƒ—ã‚’å‡¦ç†ã™ã‚‹å ´åˆã«å‚™ãˆã¦ç¶­æŒ
FINANCIAL_STATEMENT_MAPPING_DICT = {
    'maker_name': 'account', 
    'issue_date': 'amount_0', 
    'balance': 'amount_0',    
    'due_date': 'amount_1',   
    'description': 'amount_2' 
}
LOAN_DETAILS_MAPPING_DICT = {
    'maker_name': 'å€Ÿå…¥å…ˆåç§°(æ°å)',
    'issue_date': 'å€Ÿå…¥å…ˆæ‰€åœ¨åœ°(ä½æ‰€)', 
    'balance': 'æœŸæœ«ç¾åœ¨é«˜',           
    'description_rightside': 'æœŸä¸­ã®æ”¯æ‰•åˆ©å­é¡', 
    'description': 'åˆ©ç‡',            
}
# â˜…â˜…â˜… NO_HEADER_MAPPING_DICT ã¯ã‚¿ãƒ–åŒºåˆ‡ã‚Šãƒ‡ãƒ¼ã‚¿ã®ã€Œãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã€ã¨ã—ã¦å†å®šç¾©ï¼ â˜…â˜…â˜…
# ã“ã®è¾æ›¸ã¯ã€ãƒ˜ãƒƒãƒ€ãƒ¼ãªã—ã®ã‚¿ãƒ–åŒºåˆ‡ã‚Šãƒ‡ãƒ¼ã‚¿ï¼ˆãŠå®¢æ§˜æä¾›ã®ãƒ­ã‚°ã¨å…ƒãƒ‡ãƒ¼ã‚¿ä¾‹ï¼‰ã‚’èª­ã¿è¾¼ã‚€éš›ã«ä½¿ç”¨ã—ã¾ã™ã€‚
# ã‚­ãƒ¼ã¯PostgreSQLã®ã‚«ãƒ©ãƒ åã€å€¤ã¯å…ƒã®CSVãƒ•ã‚¡ã‚¤ãƒ«ã«ãŠã‘ã‚‹ã€Œ0ã‹ã‚‰å§‹ã¾ã‚‹åˆ—ç•ªå·ã€ã§ã™ã€‚
# balance ã¯å‹•çš„æ¤œå‡ºã«ä»»ã›ã‚‹ãŸã‚ã€ã“ã“ã§ã®å›ºå®šãƒãƒƒãƒ”ãƒ³ã‚°ã¯å„ªå…ˆåº¦ã‚’ä¸‹ã’ã¾ã™ã€‚
NO_HEADER_MAPPING_DICT = {
    # åŸºæœ¬æƒ…å ±ï¼ˆãƒ­ã‚°ã®0-5åˆ—ç›®ï¼‰
    'ocr_result_id': 0,
    'page_no': 1,
    'id': 2,
    'jgroupid_string': 3,
    'cif_number': 4,
    'settlement_at': 5,
    # makeræƒ…å ±ï¼ˆãƒ­ã‚°ã®6-8åˆ—ç›®ï¼‰
    'maker_name_original': 6, 
    'maker_name': 7,          
    'maker_com_code': 8,      
    # issue/due dateï¼ˆãƒ­ã‚°ã®9-12åˆ—ç›®ï¼‰
    'issue_date_original': 9,  
    'issue_date': 10,          
    'due_date_original': 11,   
    'due_date': 12,            
    # paying_bankæƒ…å ±ï¼ˆãƒ­ã‚°ã®15-16åˆ—ç›®ï¼‰
    'paying_bank_name_original': 15, 
    'paying_bank_name': 15,          
    'paying_bank_branch_name_original': 16, 
    'paying_bank_branch_name': 16,   
    # paying_bank_code ã¯å…ƒãƒ‡ãƒ¼ã‚¿ã«ãªã„ãŸã‚ãƒãƒƒãƒ”ãƒ³ã‚°ãªã— (ç©ºã®ã¾ã¾ã«ã™ã‚‹)
    # discount_bankæƒ…å ±ï¼ˆãƒ­ã‚°ã®19-20åˆ—ç›®ï¼‰
    'discount_bank_name_original': 19, 
    'discount_bank_name': 20,          
    # discount_bank_code ã¯å…ƒãƒ‡ãƒ¼ã‚¿ã«ãªã„ãŸã‚ãƒãƒƒãƒ”ãƒ³ã‚°ãªã— (ç©ºã®ã¾ã¾ã«ã™ã‚‹)
    # descriptionï¼ˆãƒ­ã‚°ã®21-22åˆ—ç›®ï¼‰
    'description_original': 21,        
    'description': 22,                 
    # registration_number ãªã©ã¯å…ƒãƒ‡ãƒ¼ã‚¿ã«ç›´æ¥å¯¾å¿œãªã—
}


# --- é–¢æ•°å®šç¾© ---
# â˜…â˜…â˜… clean_balance_no_comma é–¢æ•°ã‚’ã‚°ãƒ­ãƒ¼ãƒãƒ«ã‚¹ã‚³ãƒ¼ãƒ—ã«ç§»å‹•ï¼ â˜…â˜…â˜…
def clean_balance_no_comma(value): 
    try:
        cleaned_value = str(value).replace(',', '').replace('Â¥', '').replace('ï¿¥', '').replace('å††', '').strip() 
        if not cleaned_value:
            return '' 
        numeric_value = float(cleaned_value)
        return str(int(numeric_value)) 
    except ValueError:
        return '' 

ocr_id_mapping = {}
_ocr_id_sequence_counter = 0 
_ocr_id_fixed_timestamp_str = "" 

def get_ocr_result_id_for_group(file_group_root_name): 
    global ocr_id_mapping
    global _ocr_id_sequence_counter
    global _ocr_id_fixed_timestamp_str

    if file_group_root_name not in ocr_id_mapping:
        sequence_part_int = _ocr_id_sequence_counter * 10
        if sequence_part_int > 99999: 
            sequence_part_int = sequence_part_int % 100000 
        
        sequence_part_str = str(sequence_part_int).zfill(5) 
        
        new_ocr_id = f"{_ocr_id_fixed_timestamp_str}{sequence_part_str}" 

        ocr_id_mapping[file_group_root_name] = new_ocr_id
        _ocr_id_sequence_counter += 1
    
    return ocr_id_mapping[file_group_root_name]

maker_name_to_com_code_map = {} 
next_maker_com_code_val = 100 

def get_maker_com_code_for_name(maker_name): 
    """
    maker_nameã«åŸºã¥ã„ã¦3æ¡ã®ä¼šç¤¾ã‚³ãƒ¼ãƒ‰ã‚’æ¡ç•ªãƒ»å–å¾—ã—ã€å…ˆé ­ã« '2' ã‚’ä»˜ã‘ã¦4æ¡ã«ã™ã‚‹ã€‚
    åŒã˜maker_nameã«ã¯åŒã˜ã‚³ãƒ¼ãƒ‰ã‚’å‰²ã‚Šå½“ã¦ã‚‹ã€‚
    """
    global maker_name_to_com_code_map 
    global next_maker_com_code_val 

    maker_name_str = str(maker_name).strip() 
    
    if not maker_name_str: 
        return "" 

    if maker_name_str in maker_name_to_com_code_map:
        return maker_name_to_com_code_map[maker_name_str]
    else:
        new_code_int = next_maker_com_code_val % 1000 
        if new_code_int < 100: 
            new_code_int = 100 + new_code_int 
        
        new_code_4digit = '2' + str(new_code_int).zfill(3) 
        
        maker_name_to_com_code_map[maker_name_str] = new_code_4digit 
        next_maker_com_code_val += 1
        return new_code_4digit

# â˜…â˜…â˜… æ–°è¦è¿½åŠ ï¼šé‡‘é¡ã‚‰ã—ã„åˆ—ã‹ã©ã†ã‹ã‚’åˆ¤å®šã™ã‚‹é–¢æ•°ï¼ˆãŠå®¢æ§˜ã®æä¾›ã‚’å‚è€ƒã«ï¼‰ â˜…â˜…â˜…
def is_likely_amount_column(series):
    """é‡‘é¡ã‚‰ã—ã„åˆ—ã‹ã©ã†ã‹ã‚’åˆ¤å®šã™ã‚‹é–¢æ•°"""
    if not pd.api.types.is_string_dtype(series): 
        series = series.astype(str)
    
    cleaned_series = series.dropna().astype(str).str.replace(r'[Â¥ï¿¥,å††\sã€€]', '', regex=True)
    
    if cleaned_series.empty:
        return False 

    patterns = [r'^\d{1,3}(,\d{3})*(\.\d+)?$', r'^\d+å††$', r'^[\d,]+$', r'^\d+\.\d{2}$', r'^[+-]?\d+$'] 
    
    match_count = 0
    for val in cleaned_series:
        if any(re.fullmatch(p, val) for p in patterns): 
            match_count += 1
    
    return match_count >= max(1, len(cleaned_series) * 0.5) 

# â˜…â˜…â˜… æ–°è¦è¿½åŠ ï¼šé‡‘é¡åˆ—ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’ç‰¹å®šã™ã‚‹é–¢æ•°ï¼ˆãŠå®¢æ§˜ã®æä¾›ã‚’å‚è€ƒã«ï¼‰ â˜…â˜…â˜…
def detect_amount_column_index(df):
    """DataFrameã‹ã‚‰é‡‘é¡åˆ—ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’ç‰¹å®šã™ã‚‹"""
    potential_amount_cols = []
    # å…¨ã¦ã®ã‚«ãƒ©ãƒ ã‚’ãƒã‚§ãƒƒã‚¯ã™ã‚‹ãŒã€å¾ŒåŠã®æ–¹ã«é‡‘é¡ãŒã‚ã‚‹å¯èƒ½æ€§ãŒé«˜ã„ãŸã‚ã€å¾Œã‚ã‹ã‚‰æ¢ã™ã“ã¨ã‚‚è€ƒæ…®
    for i in range(df.shape[1] -1, -1, -1): # å¾Œã‚ã‹ã‚‰èµ°æŸ»
        col = df.columns[i]
        if is_likely_amount_column(df[col]):
            numeric_values = df[col].astype(str).str.replace(r'[Â¥ï¿¥,å††\sã€€]', '', regex=True).apply(lambda x: pd.to_numeric(x, errors='coerce'))
            if not numeric_values.isnull().all(): 
                potential_amount_cols.append((i, numeric_values.sum())) 
    
    if not potential_amount_cols:
        return -1 

    potential_amount_cols.sort(key=lambda x: x[1], reverse=True)
    return potential_amount_cols[0][0] 


def process_universal_csv(input_filepath, processed_output_base_dir, input_base_dir, 
                        maker_master_df, ocr_id_map_for_groups, current_file_group_root_name, 
                        final_postgre_columns_list, no_header_map, hand_bill_map, financial_map, loan_map,
                        accounts_receivable_map=None): 
    """
    å…¨ã¦ã®AIReadå‡ºåŠ›CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿ã€çµ±ä¸€ã•ã‚ŒãŸPostgreSQLå‘ã‘ã‚«ãƒ©ãƒ å½¢å¼ã«å¤‰æ›ã—ã¦å‡ºåŠ›ã™ã‚‹é–¢æ•°ã€‚
    CSVã®ç¨®é¡ï¼ˆãƒ˜ãƒƒãƒ€ãƒ¼å†…å®¹ï¼‰ã‚’åˆ¤åˆ¥ã—ã€ãã‚Œãã‚Œã«å¿œã˜ãŸãƒãƒƒãƒ”ãƒ³ã‚°ã‚’é©ç”¨ã™ã‚‹ã€‚
    """
    df_original = None
    file_type = "ä¸æ˜" 
    
    try:
        encodings_to_try = ['utf-8', 'utf-8-sig', 'shift_jis', 'cp932']
        
        for enc in encodings_to_try:
            try:
                # 1. ãƒ˜ãƒƒãƒ€ãƒ¼ã‚ã‚Šã€ã‚«ãƒ³ãƒåŒºåˆ‡ã‚Šã§èª­ã¿è¾¼ã¿ã‚’è©¦ã™ (æ‰‹å½¢æƒ…å ±ãªã©ä¸€èˆ¬çš„ãªCSVå½¢å¼)
                df_temp_comma_header = pd.read_csv(input_filepath, encoding=enc, header=0, sep=',', quotechar='"', 
                                                    dtype=str, na_values=['ã€ƒ'], keep_default_na=False)
                df_temp_comma_header.columns = df_temp_comma_header.columns.str.strip() 
                current_headers_comma = df_temp_comma_header.columns.tolist()

                is_hand_bill = ('æŒ¯å‡ºäºº' in current_headers_comma) and ('é‡‘é¡' in current_headers_comma)
                is_financial = ('account' in current_headers_comma)
                is_loan = ('å€Ÿå…¥å…ˆåç§°(æ°å)' in current_headers_comma)
                
                if is_hand_bill:
                    df_original = df_temp_comma_header.copy()
                    file_type = "æ‰‹å½¢æƒ…å ±"
                elif is_financial:
                    df_original = df_temp_comma_header.copy()
                    file_type = "è²¡å‹™è«¸è¡¨"
                elif is_loan:
                    df_original = df_temp_comma_header.copy()
                    file_type = "å€Ÿå…¥é‡‘æ˜ç´°"
                else: 
                    # 2. ãƒ˜ãƒƒãƒ€ãƒ¼ãªã—ã€ã‚¿ãƒ–åŒºåˆ‡ã‚Šã§èª­ã¿è¾¼ã¿ã‚’è©¦ã™ (æ±ç”¨ãƒ‡ãƒ¼ã‚¿_ãƒ˜ãƒƒãƒ€ãƒ¼ãªã—ã®å¯èƒ½æ€§ãŒé«˜ã„)
                    try:
                        df_temp_tab_noheader = pd.read_csv(input_filepath, encoding=enc, header=None, sep='\t', quotechar='"', 
                                                        dtype=str, na_values=['ã€ƒ'], keep_default_na=False)
                        df_temp_tab_noheader.columns = df_temp_tab_noheader.columns.astype(str).str.strip()
                        
                        # æ±ç”¨ãƒ‡ãƒ¼ã‚¿ã¨åˆ¤å®šã™ã‚‹åŸºæº–: ã‚¿ãƒ–åŒºåˆ‡ã‚Šã§èª­ã¿è¾¼ã‚ã¦ã€ã‹ã¤ã‚ã‚‹ç¨‹åº¦ã®åˆ—æ•°ãŒã‚ã‚‹ã“ã¨
                        max_idx_no_header_map = max(no_header_map.values()) if no_header_map else 0
                        
                        if df_temp_tab_noheader.shape[1] > max_idx_no_header_map: 
                            file_type = "æ±ç”¨ãƒ‡ãƒ¼ã‚¿_ãƒ˜ãƒƒãƒ€ãƒ¼ãªã—"
                            df_original = df_temp_tab_noheader.copy()
                        else: # åˆ—æ•°ãŒå°‘ãªã„å ´åˆã¯ã€ã‚«ãƒ³ãƒåŒºåˆ‡ã‚Šãƒ˜ãƒƒãƒ€ãƒ¼ãªã—ã®å†è©¦è¡Œã¸
                            raise ValueError("ã‚¿ãƒ–åŒºåˆ‡ã‚Šãƒ‡ãƒ¼ã‚¿ãŒæœŸå¾…ã™ã‚‹åˆ—æ•°ã«æº€ãŸãªã„") 

                    except Exception as e_tab:
                        # 3. ã‚¿ãƒ–åŒºåˆ‡ã‚Šã§ã‚‚å¤±æ•—ã—ãŸã‚‰ã€ã‚«ãƒ³ãƒåŒºåˆ‡ã‚Šã€ãƒ˜ãƒƒãƒ€ãƒ¼ãªã—ã§å†è©¦è¡Œï¼ˆæœ€çµ‚ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰
                        print(f"  ãƒ•ã‚¡ã‚¤ãƒ« {os.path.basename(input_filepath)} ã‚’ {enc} ã§ã‚¿ãƒ–åŒºåˆ‡ã‚Šèª­ã¿è¾¼ã¿å¤±æ•—ã€‚ã‚«ãƒ³ãƒåŒºåˆ‡ã‚Šã‚’è©¦ã—ã¾ã™ã€‚ã‚¨ãƒ©ãƒ¼: {e_tab}")
                        file_type = "æ±ç”¨ãƒ‡ãƒ¼ã‚¿_ãƒ˜ãƒƒãƒ€ãƒ¼ãªã—" # ã“ã®å ´åˆã¯å®Ÿéš›ã«ã¯æ±ç”¨ãƒ‡ãƒ¼ã‚¿ã«ãªã‚‹ã¯ãš
                        df_original = pd.read_csv(input_filepath, encoding=enc, header=None, sep=',', quotechar='"', 
                                                dtype=str, na_values=['ã€ƒ'], keep_default_na=False)
                        df_original.columns = df_original.columns.astype(str).str.strip() 
                
                print(f"  ãƒ‡ãƒãƒƒã‚°: ãƒ•ã‚¡ã‚¤ãƒ« {os.path.basename(input_filepath)} ã®åˆ¤å®šçµæœ: '{file_type}'")
                print(f"  ãƒ‡ãƒãƒƒã‚°: èª­ã¿è¾¼ã‚“ã  df_original ã®ã‚«ãƒ©ãƒ :\n{df_original.columns.tolist()}")
                print(f"  ãƒ‡ãƒãƒƒã‚°: èª­ã¿è¾¼ã‚“ã  df_original ã®æœ€åˆã®3è¡Œ:\n{df_original.head(3).to_string()}") 
                print(f"  ãƒ‡ãƒãƒƒã‚°: df_originalå†…ã®æ¬ æå€¤ (NaN) ã®æ•°:\n{df_original.isnull().sum().to_string()}") 
                    
                break 
            except Exception as e_inner: 
                print(f"  ãƒ•ã‚¡ã‚¤ãƒ« {os.path.basename(input_filepath)} ã‚’ {enc} ã§èª­ã¿è¾¼ã¿å¤±æ•—ã€‚åˆ¥ã®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚’è©¦ã—ã¾ã™ã€‚ã‚¨ãƒ©ãƒ¼: {e_inner}")
                df_original = None 
                continue 

        if df_original is None or df_original.empty:
            print(f"  è­¦å‘Š: ãƒ•ã‚¡ã‚¤ãƒ« {os.path.basename(input_filepath)} ã‚’ã©ã®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã¨ãƒ˜ãƒƒãƒ€ãƒ¼è¨­å®šã§ã‚‚èª­ã¿è¾¼ã‚ã¾ã›ã‚“ã§ã—ãŸã€‚å‡¦ç†ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
            return 
        
        print(f"  ãƒ•ã‚¡ã‚¤ãƒ« {os.path.basename(input_filepath)} ã¯ '{file_type}' ã¨ã—ã¦å‡¦ç†ã—ã¾ã™ã€‚")

    except Exception as e:
        print(f"âŒ ã‚¨ãƒ©ãƒ¼ç™ºç”Ÿï¼ˆ{input_filepath}ï¼‰: CSVèª­ã¿è¾¼ã¿ã¾ãŸã¯ãƒ•ã‚¡ã‚¤ãƒ«ã‚¿ã‚¤ãƒ—åˆ¤åˆ¥ã§å•é¡ŒãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚ã‚¨ãƒ©ãƒ¼: {e}")
        import traceback
        traceback.print_exc()
        return

    # --- ãƒ‡ãƒ¼ã‚¿åŠ å·¥å‡¦ç† ---
    df_data_rows = df_original.copy() 

    if df_data_rows.empty:
        print(f"  è­¦å‘Š: ãƒ•ã‚¡ã‚¤ãƒ« {os.path.basename(input_filepath)} ã«æœ‰åŠ¹ãªãƒ‡ãƒ¼ã‚¿è¡ŒãŒè¦‹ã¤ã‹ã‚‰ãªã‹ã£ãŸãŸã‚ã€åŠ å·¥ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
        return 

    # ã€Œã€ƒã€ãƒãƒ¼ã‚¯ã®ã¿ã‚’ffillã§åŸ‹ã‚ã€ç©ºæ–‡å­—åˆ—ã¯ãã®ã¾ã¾ç¶­æŒ
    df_data_rows = df_data_rows.ffill() 
    df_data_rows = df_data_rows.fillna('') 
    print(f"  â„¹ï¸ ã€Œã€ƒã€ãƒãƒ¼ã‚¯ã‚’ç›´ä¸Šãƒ‡ãƒ¼ã‚¿ã§åŸ‹ã‚ã€å…ƒã€…ãƒ–ãƒ©ãƒ³ã‚¯ã ã£ãŸç®‡æ‰€ã¯ç¶­æŒã—ã¾ã—ãŸã€‚")

    # åˆè¨ˆè¡Œã®å‰Šé™¤ãƒ­ã‚¸ãƒƒã‚¯
    keywords_to_delete = ["åˆè¨ˆ", "å°è¨ˆ", "è¨ˆ", "æ‰‹æŒæ‰‹å½¢è¨ˆ", "å‰²å¼•æ‰‹å½¢è¨ˆ"] 
    
    filter_conditions = []
    keywords_regex = r'|'.join([re.escape(k) for k in keywords_to_delete]) 
    
    if file_type == "æ‰‹å½¢æƒ…å ±":
        if 'æŒ¯å‡ºäºº' in df_data_rows.columns:
            filter_conditions.append(df_data_rows['æŒ¯å‡ºäºº'].str.contains(keywords_regex, regex=True, na=False))
    elif file_type == "è²¡å‹™è«¸è¡¨": 
        if 'account' in df_data_rows.columns:
            filter_conditions.append(df_data_rows['account'].str.contains(keywords_regex, regex=True, na=False))
    elif file_type == "å€Ÿå…¥é‡‘æ˜ç´°": 
        if 'å€Ÿå…¥å…ˆåç§°(æ°å)' in df_data_rows.columns:
            filter_conditions.append(df_data_rows['å€Ÿå…¥å…ˆåç§°(æ°å)'].str.contains(keywords_regex, regex=True, na=False))
    elif file_type == "æ±ç”¨ãƒ‡ãƒ¼ã‚¿_ãƒ˜ãƒƒãƒ€ãƒ¼ãªã—": 
        # 'æ±ç”¨ãƒ‡ãƒ¼ã‚¿_ãƒ˜ãƒƒãƒ€ãƒ¼ãªã—' ã®å ´åˆã€ maker_name ã¯ NO_HEADER_MAPPING_DICT ã® maker_name ã«å¯¾å¿œã™ã‚‹åˆ—ã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚’è¦‹ã‚‹
        if 'maker_name' in no_header_map and str(no_header_map['maker_name']) in df_data_rows.columns: 
            filter_conditions.append(df_data_rows[str(no_header_map['maker_name'])].str.contains(keywords_regex, regex=True, na=False))
        elif '0' in df_data_rows.columns: # æœ€æ‚ª0åˆ—ç›®å…¨ä½“ã§ãƒã‚§ãƒƒã‚¯
            filter_conditions.append(df_data_rows['0'].str.contains(keywords_regex, regex=True, na=False))

    if filter_conditions:
        combined_filter = pd.concat(filter_conditions, axis=1).any(axis=1)
        rows_deleted_count = combined_filter.sum()
        df_data_rows = df_data_rows[~combined_filter].reset_index(drop=True)
        if rows_deleted_count > 0:
            print(f"  â„¹ï¸ åˆè¨ˆè¡Œï¼ˆã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒ‘ã‚¿ãƒ¼ãƒ³: {keywords_regex}ï¼‰ã‚’ {rows_deleted_count} è¡Œå‰Šé™¤ã—ã¾ã—ãŸã€‚")
    
    num_rows_to_process = len(df_data_rows) 
    
    # df_processed ã®åˆæœŸåŒ–
    df_processed = pd.DataFrame('', index=range(num_rows_to_process), columns=final_postgre_columns_list)


    # --- å…±é€šé …ç›® (PostgreSQLã®ã‚°ãƒªãƒ¼ãƒ³ã®è¡¨ã®å·¦å´ã€è‡ªå‹•ç”Ÿæˆé …ç›®) ã‚’ç”Ÿæˆ ---
    df_processed['ocr_result_id'] = [get_ocr_result_id_for_group(current_file_group_root_name)] * num_rows_to_process 

    df_processed['page_no'] = [1] * num_rows_to_process 

    df_processed['id'] = range(1, num_rows_to_process + 1)

    df_processed['jgroupid_string'] = ['001'] * num_rows_to_process

    cif_number_val = current_file_group_root_name[1:] 
    df_processed['cif_number'] = [cif_number_val] * num_rows_to_process

    settlement_at_val = datetime.now().strftime('%Y%m') 
    df_processed['settlement_at'] = [settlement_at_val] * num_rows_to_process


    # --- å„ãƒ•ã‚¡ã‚¤ãƒ«ã‚¿ã‚¤ãƒ—ã«å¿œã˜ãŸãƒãƒƒãƒ”ãƒ³ã‚°ãƒ«ãƒ¼ãƒ«ã‚’é©ç”¨ ---
    mapping_to_use = {}
    if file_type == "æ‰‹å½¢æƒ…å ±": 
        mapping_to_use = hand_bill_map
    elif file_type == "è²¡å‹™è«¸è¡¨": 
        mapping_to_use = financial_map
    elif file_type == "å€Ÿå…¥é‡‘æ˜ç´°": 
        mapping_to_use = loan_map
    else: # æ±ç”¨ãƒ‡ãƒ¼ã‚¿_ãƒ˜ãƒƒãƒ€ãƒ¼ãªã— ã®å ´åˆ
        mapping_to_use = NO_HEADER_MAPPING_DICT 

    df_data_rows.columns = df_data_rows.columns.astype(str) # å¿µã®ãŸã‚strã«å¤‰æ›
    
    # â˜…â˜…â˜… ãƒãƒƒãƒ”ãƒ³ã‚°å‡¦ç†ï¼šå…ƒã®CSVãƒ‡ãƒ¼ã‚¿ã‚’PostgreSQLã‚«ãƒ©ãƒ ã«ã‚³ãƒ”ãƒ¼ï¼ˆã€Œâ˜…ä»Šã®ã¾ã¾ã€ã«å¯¾å¿œï¼‰ â˜…â˜…â˜…
    # df_processed ã‚’æœ€çµ‚ã‚«ãƒ©ãƒ ãƒªã‚¹ãƒˆã§ç©ºæ–‡å­—åˆ—ã§åˆæœŸåŒ–ã—ãŸå¾Œã€
    # å…ƒã®CSVãƒ‡ãƒ¼ã‚¿ï¼ˆdf_data_rowsï¼‰ã‹ã‚‰ã€PostgreSQLã®æœ€çµ‚ã‚«ãƒ©ãƒ ã«ç›´æ¥å¯¾å¿œã™ã‚‹ãƒ‡ãƒ¼ã‚¿ã‚’ã‚³ãƒ”ãƒ¼ã™ã‚‹ã€‚
    # ã“ã‚Œã«ã‚ˆã‚Šã€ã€Œâ˜…ä»Šã®ã¾ã¾ã€ã®ã‚«ãƒ©ãƒ ã®ãƒ‡ãƒ¼ã‚¿ãŒã€ä»–ã®ãƒ­ã‚¸ãƒƒã‚¯ã§ä¸Šæ›¸ãã•ã‚Œã‚‹å‰ã«ç¢ºå®Ÿã«ä¿æŒã•ã‚Œã‚‹ã€‚
    
    # ã“ã®ãƒ«ãƒ¼ãƒ—ã¯ã€df_original.columns ã®ã€Œç¾åœ¨ã®ã‚«ãƒ©ãƒ åã€ã‚’ç›´æ¥åˆ©ç”¨ã—ã¦ã‚³ãƒ”ãƒ¼
    # æ±ç”¨ãƒ‡ãƒ¼ã‚¿ã®å ´åˆã€ã‚«ãƒ©ãƒ åã¯ '0', '1', ... ã¨ãªã‚‹ã®ã§ã€ã“ã®ãƒ«ãƒ¼ãƒ—ã§ã¯ç›´æ¥ã‚³ãƒ”ãƒ¼ã•ã‚Œãªã„ã‚«ãƒ©ãƒ ãŒå¤šã„
    for col_name_in_original_df in df_data_rows.columns: 
        if col_name_in_original_df in final_postgre_columns_list:
            df_processed[col_name_in_original_df] = df_data_rows[col_name_in_original_df].copy()

    # ãƒãƒƒãƒ”ãƒ³ã‚°è¾æ›¸ï¼ˆhand_bill_mapãªã©ï¼‰ã‚’é©ç”¨ã™ã‚‹
    # ã“ã®ãƒ«ãƒ¼ãƒ—ã¯ã€ãƒ•ã‚¡ã‚¤ãƒ«ã‚¿ã‚¤ãƒ—ã«å¿œã˜ãŸã€Œç¿»è¨³è¡¨ã€ã‚’ä½¿ã£ã¦ã€ãƒ‡ãƒ¼ã‚¿ã‚’ã‚ˆã‚Šé©åˆ‡ãª Postgre ã‚«ãƒ©ãƒ ã«ç§»å‹•ã•ã›ã‚‹
    # ä¾‹ï¼šhand_bill_mapã® 'æŒ¯å‡ºäºº' -> 'maker_name'
    # NO_HEADER_MAPPING_DICT ã®å ´åˆã¯ src_ref ãŒæ•°å€¤ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã®æ–‡å­—åˆ— ('0','1',...) ã«ãªã‚‹ã€‚
    for pg_col_name, src_ref in mapping_to_use.items():
        # df_processed[pg_col_name] ã«æ—¢ã«å€¤ãŒå…¥ã£ã¦ã„ã‚‹å ´åˆï¼ˆä¸Šã®ã€Œâ˜…ä»Šã®ã¾ã¾ã€ãƒ­ã‚¸ãƒƒã‚¯ã§ã‚³ãƒ”ãƒ¼ã•ã‚ŒãŸå ´åˆï¼‰ã¯ã€
        # ãã®å€¤ã‚’å°Šé‡ã—ã€ãƒãƒƒãƒ”ãƒ³ã‚°è¾æ›¸ã‹ã‚‰ã®ä¸Šæ›¸ãã¯è¡Œã‚ãªã„ã‚ˆã†ã«ã™ã‚‹ï¼ˆãŠå®¢æ§˜ã®ã€Œä»Šã®ã¾ã¾ã€ã‚’å³å®ˆï¼‰ã€‚
        # ãŸã ã—ã€ãƒ–ãƒ©ãƒ³ã‚¯ã§ã‚ã‚‹å ´åˆã¯ä¸Šæ›¸ãã‚’è¨±å¯ã™ã‚‹ã€‚
        if df_processed[pg_col_name].isin(['', None, np.nan]).all(): # df_processedã®ã‚«ãƒ©ãƒ ãŒå…¨ã¦ç©ºã®å ´åˆ
            source_data_series = None
            if isinstance(src_ref, str): # ãƒ˜ãƒƒãƒ€ãƒ¼åã§ãƒãƒƒãƒ”ãƒ³ã‚° (æ‰‹å½¢æƒ…å ±ãªã©)
                if src_ref in df_data_rows.columns: 
                    source_data_series = df_data_rows[src_ref]
            elif isinstance(src_ref, int): # åˆ—ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã§ãƒãƒƒãƒ”ãƒ³ã‚° (æ±ç”¨ãƒ‡ãƒ¼ã‚¿_ãƒ˜ãƒƒãƒ€ãƒ¼ãªã—ç”¨)
                if str(src_ref) in df_data_rows.columns: 
                    source_data_series = df_data_rows[str(src_ref)]
            
            if source_data_series is not None:
                df_processed[pg_col_name] = source_data_series.astype(str).values 
            else:
                pass 


    # â˜…â˜…â˜… é‡‘é¡ã‚«ãƒ©ãƒ ã®å‹•çš„æ¤œå‡ºãƒ­ã‚¸ãƒƒã‚¯ã‚’è¿½åŠ ï¼ â˜…â˜…â˜…
    # æ±ç”¨ãƒ‡ãƒ¼ã‚¿ã®å ´åˆã®ã¿ã€é‡‘é¡ã‚’è‡ªå‹•æ¤œå‡ºã—ã¦åŸ‹ã‚ã‚‹
    if file_type == "æ±ç”¨ãƒ‡ãƒ¼ã‚¿_ãƒ˜ãƒƒãƒ€ãƒ¼ãªã—":
        amount_col_idx = detect_amount_column_index(df_data_rows)
        if amount_col_idx != -1:
            # balance_original ã¯é‡‘é¡ã‚«ãƒ©ãƒ ã®1ã¤å‰ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã®ãƒ‡ãƒ¼ã‚¿ã€balance ã¯é‡‘é¡ã‚«ãƒ©ãƒ è‡ªä½“ã®ãƒ‡ãƒ¼ã‚¿
            # ã“ã‚Œã¯ä¸€èˆ¬çš„ãªã‚±ãƒ¼ã‚¹ã§ã‚ã‚Šã€ãŠå®¢æ§˜ã®ãƒ‡ãƒ¼ã‚¿ä¾‹ã§é‡‘é¡ãŒ2åˆ—ä¸¦ã‚“ã§ã„ã‚‹ã‚±ãƒ¼ã‚¹ã«å¯¾å¿œ
            raw_balance_series = df_data_rows.iloc[:, amount_col_idx].astype(str) # é‡‘é¡ã‚«ãƒ©ãƒ è‡ªä½“
            # balance_original ã‚‚ balance ã¨åŒã˜å€¤ã‚’ã‚¯ãƒªãƒ¼ãƒ³ã—ã¦ã‚³ãƒ”ãƒ¼
            df_processed['balance'] = raw_balance_series.apply(clean_balance_no_comma)
            df_processed['balance_original'] = df_processed['balance'].copy() # balanceã‹ã‚‰ã‚³ãƒ”ãƒ¼ã™ã‚‹
            print(f"  â„¹ï¸ é‡‘é¡ã‚«ãƒ©ãƒ ã‚’åˆ—ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ '{amount_col_idx}' ã‹ã‚‰å‹•çš„ã«æ¤œå‡ºã—ã¾ã—ãŸã€‚")
        else:
            print("  âš ï¸ è­¦å‘Š: é‡‘é¡ã‚«ãƒ©ãƒ ã‚’å‹•çš„ã«æ¤œå‡ºã§ãã¾ã›ã‚“ã§ã—ãŸã€‚balanceã‚«ãƒ©ãƒ ã¯ãƒ–ãƒ©ãƒ³ã‚¯ã®ã¾ã¾ã§ã™ã€‚")


    # --- Excelé–¢æ•°ç›¸å½“ã®ãƒ­ã‚¸ãƒƒã‚¯ã‚’é©ç”¨ï¼ˆæ´¾ç”Ÿã‚«ãƒ©ãƒ ã®ç”Ÿæˆï¼‰ ---
    # â˜…â˜…â˜… å„ã‚«ãƒ©ãƒ ã®ç”Ÿæˆãƒ­ã‚¸ãƒƒã‚¯ã‚’ãŠå®¢æ§˜ãŒæç¤ºã—ãŸæœ€æ–°ã®79ã‚«ãƒ©ãƒ ãƒªã‚¹ãƒˆã«å¿ å®Ÿã«å†ç¾ã™ã‚‹ï¼ â˜…â˜…â˜…
    
    # registration_number_original, registration_number (ãƒ–ãƒ©ãƒ³ã‚¯)
    df_processed['registration_number_original'] = '' 
    df_processed['registration_number'] = '' 

    # maker_name_original, maker_name
    df_processed['maker_name_original'] = df_processed['maker_name'].copy() 
    
    # maker_com_code 
    df_processed['maker_com_code'] = df_processed['maker_name'].apply(get_maker_com_code_for_name)
    
    # maker_com_code_status_id, maker_comcd_relation_source_type_id, maker_exist_comcd_relation_history_id (å›ºå®šå€¤)
    df_processed['maker_com_code_status_id'] = '30'
    df_processed['maker_comcd_relation_source_type_id'] = '30'
    df_processed['maker_exist_comcd_relation_history_id'] = '20'

    # issue_date_original, issue_date, due_date_original, due_date
    df_processed['issue_date_original'] = df_processed['issue_date'].copy() 
    df_processed['due_date_original'] = df_processed['due_date'].copy()   

    # paying_bank_name_original, paying_bank_name, paying_bank_branch_name_original, paying_bank_branch_name
    # discount_bank_name_original, discount_bank_name
    df_processed['paying_bank_name_original'] = df_processed['paying_bank_name'].copy() 
    df_processed['paying_bank_branch_name_original'] = df_processed['paying_bank_branch_name'].copy() 
    df_processed['discount_bank_name_original'] = df_processed['discount_bank_name'].copy() 

    # paying_bank_code, discount_bank_code (ãƒ–ãƒ©ãƒ³ã‚¯)
    df_processed['paying_bank_code'] = '' 
    df_processed['discount_bank_code'] = '' 

    # balance_original, balance
    # clean_balance_no_comma é–¢æ•°ã¯æ—¢ã«ã‚°ãƒ­ãƒ¼ãƒãƒ«ã«å®šç¾©ã•ã‚Œã¦ã„ã‚‹
    # df_processed['balance'] ã¨ df_processed['balance_original'] ã¯å‹•çš„æ¤œå‡ºã§æ—¢ã«è¨­å®šã•ã‚Œã¦ã„ã‚‹å¯èƒ½æ€§ãŒã‚ã‚‹ã®ã§ã€ä¸Šæ›¸ãã—ãªã„
    # ã‚‚ã—å‹•çš„æ¤œå‡ºã§è¨­å®šã•ã‚Œãªã‹ã£ãŸå ´åˆï¼ˆamount_col_idx == -1ï¼‰ã¯ã€åˆæœŸå€¤ã®ç©ºæ–‡å­—åˆ—ã®ã¾ã¾ã¨ãªã‚‹
    
    # description_original, description
    df_processed['description_original'] = df_processed['description'].copy() 
    
    # conf_ç³» (å›ºå®šå€¤)
    df_processed['conf_registration_number'] = '100'
    df_processed['conf_maker_name'] = '100'
    df_processed['conf_issue_date'] = '100'
    df_processed['conf_due_date'] = '100'
    df_processed['conf_balance'] = '100'
    df_processed['conf_paying_bank_name'] = '100'
    df_processed['conf_paying_bank_branch_name'] = '100'
    df_processed['conf_discount_bank_name'] = '100'
    df_processed['conf_description'] = '100'

    # coord_ç³» (å›ºå®šå€¤)
    df_processed['coord_x_registration_number'] = '3000'
    df_processed['coord_y_registration_number'] = '3000'
    df_processed['coord_h_registration_number'] = '3000'
    df_processed['coord_w_registration_number'] = '3000'
    df_processed['coord_x_maker_name'] = '3000'
    df_processed['coord_y_maker_name'] = '3000'
    df_processed['coord_h_maker_name'] = '3000'
    df_processed['coord_w_maker_name'] = '3000'
    df_processed['coord_x_issue_date'] = '3000'
    df_processed['coord_y_issue_date'] = '3000'
    df_processed['coord_h_issue_date'] = '3000'
    df_processed['coord_w_issue_date'] = '3000'
    df_processed['coord_x_due_date'] = '3000'
    df_processed['coord_y_due_date'] = '3000'
    df_processed['coord_h_due_date'] = '3000'
    df_processed['coord_w_due_date'] = '3000'
    df_processed['coord_x_balance'] = '3000'
    df_processed['coord_y_balance'] = '3000'
    df_processed['coord_h_balance'] = '3000'
    df_processed['coord_w_balance'] = '3000'
    df_processed['coord_x_paying_bank_name'] = '3000'
    df_processed['coord_y_paying_bank_name'] = '3000'
    df_processed['coord_h_paying_bank_name'] = '3000'
    df_processed['coord_w_paying_bank_name'] = '3000'
    df_processed['coord_x_paying_bank_branch_name'] = '3000'
    df_processed['coord_y_paying_bank_branch_name'] = '3000'
    df_processed['coord_h_paying_bank_branch_name'] = '3000'
    df_processed['coord_w_paying_bank_branch_name'] = '3000'
    df_processed['coord_x_discount_bank_name'] = '3000'
    df_processed['coord_y_discount_bank_name'] = '3000'
    df_processed['coord_h_discount_bank_name'] = '3000'
    df_processed['coord_w_discount_bank_name'] = '3000'
    df_processed['coord_x_description'] = '3000'
    df_processed['coord_y_description'] = '3000'
    df_processed['coord_h_description'] = '3000'
    df_processed['coord_w_description'] = '3000'

    df_processed['row_no'] = range(1, num_rows_to_process + 1) # row_no ã®ç”Ÿæˆãƒ­ã‚¸ãƒƒã‚¯
    df_processed['insertdatetime'] = '' 
    df_processed['updatedatetime'] = '' 
    df_processed['updateuser'] = 'testuser' 
    
    # balanceãŒç©ºã§ãªã„å ´åˆã«balance_originalã«balanceã®å€¤ã‚’ã‚³ãƒ”ãƒ¼ã—ã¾ã™ã€‚	
    df_processed.loc[df_processed['balance'].astype(str).str.strip() != '', 'balance_original'] = df_processed['balance']
    
    # --- ä¿å­˜å‡¦ç† ---
    relative_path_to_file = os.path.relpath(input_filepath, input_base_dir)
    relative_dir_to_file = os.path.dirname(relative_path_to_file)
    processed_output_sub_dir = os.path.join(processed_output_base_dir, relative_dir_to_file)
    
    processed_output_filename = os.path.basename(input_filepath).replace('.csv', '_processed.csv')
    processed_output_filepath = os.path.join(processed_output_sub_dir, processed_output_filename) 
    
    os.makedirs(processed_output_sub_dir, exist_ok=True) 
    df_processed.to_csv(processed_output_filepath, index=False, encoding='utf-8-sig')

    print(f"âœ… åŠ å·¥å®Œäº†: {input_filepath} -> {processed_output_filepath}")

# --- ãƒ¡ã‚¤ãƒ³å‡¦ç† ---
if __name__ == "__main__":
    print(f"--- å‡¦ç†é–‹å§‹: {datetime.now()} ({APP_ROOT_DIR}) ---") 
    
    _ocr_id_fixed_timestamp_str = datetime.now().strftime('%Y%m%d%H%M')
    print(f"  â„¹ï¸ OCR IDç”Ÿæˆã®å›ºå®šæ™‚åˆ»: {_ocr_id_fixed_timestamp_str}")

    os.makedirs(PROCESSED_OUTPUT_BASE_DIR, exist_ok=True) 

    MASTER_DATA_DIR = os.path.join(APP_ROOT_DIR, 'master_data') 
    os.makedirs(MASTER_DATA_DIR, exist_ok=True) 

    maker_master_filepath = os.path.join(MASTER_DATA_DIR, 'master.csv') 
    maker_master_df = pd.DataFrame() 
    if os.path.exists(maker_master_filepath):
        try:
            maker_master_df = pd.read_csv(maker_master_filepath, encoding='utf-8')
            print(f"  â„¹ï¸ {maker_master_filepath} ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ (ã“ã®ãƒ‡ãƒ¼ã‚¿ã¯maker_com_codeç”Ÿæˆã«åˆ©ç”¨ã•ã‚Œã¾ã™)ã€‚")
        except Exception as e:
            print(f"âŒ ã‚¨ãƒ©ãƒ¼: {maker_master_filepath} ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸã€‚ã‚¨ãƒ©ãƒ¼: {e}")
            maker_master_df = pd.DataFrame() 
    else:
        print(f"âš ï¸ è­¦å‘Š: {maker_master_filepath} ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ (maker_com_codeç”Ÿæˆã«å½±éŸ¿ã™ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™)ã€‚")
        maker_master_df = pd.DataFrame() 


    jgroupid_master_filepath = os.path.join(MASTER_DATA_DIR, 'jgroupid_master.csv')
    jgroupid_values_from_master = [] 
    if os.path.exists(jgroupid_master_filepath): 
        try:
            df_jgroupid_temp = pd.read_csv(jgroupid_master_filepath, encoding='utf-8', header=None)
            
            if not df_jgroupid_temp.empty and df_jgroupid_temp.shape[1] > 0:
                jgroupid_values_from_master = df_jgroupid_temp.iloc[:, 0].astype(str).tolist()
                if not jgroupid_values_from_master:
                    raise ValueError("jgroupid_master.csv ã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã‚ã¾ã—ãŸãŒã€ãƒªã‚¹ãƒˆãŒç©ºã§ã™ã€‚")
            else:
                raise ValueError("jgroupid_master.csv ãŒç©ºã¾ãŸã¯ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
            
        except Exception as e:
            print(f"âŒ ã‚¨ãƒ©ãƒ¼: jgroupid_master.csv ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸã€‚ã‚¨ãƒ©ãƒ¼: {e}")
            jgroupid_values_from_master = [] 
    else:
        print(f"âš ï¸ è­¦å‘Š: {jgroupid_master_filepath} ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚ãƒ‘ã‚¹ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
        jgroupid_values_from_master = [str(i).zfill(3) for i in range(1, 94)] # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã§åˆæœŸåŒ–ã—ã¦ãŠã

    INPUT_CSV_FILES_DIR = INPUT_BASE_DIR 

    # ocr_result_id ã®ãƒãƒƒãƒ”ãƒ³ã‚°ã‚’äº‹å‰ã«ç”Ÿæˆã™ã‚‹ãƒ­ã‚¸ãƒƒã‚¯
    print("\n--- ocr_result_id ãƒãƒƒãƒ”ãƒ³ã‚°äº‹å‰ç”Ÿæˆé–‹å§‹ ---")
    ocr_id_mapping = {}
    _ocr_id_sequence_counter = 0 
    
    all_target_file_groups_root = set() 
    for root, dirs, files in os.walk(INPUT_CSV_FILES_DIR): 
        for filename in files:
            if filename.lower().endswith('.csv') and not filename.lower().endswith('_processed.csv'):
                # ãƒ•ã‚¡ã‚¤ãƒ«åã‹ã‚‰ã€Œãƒ•ã‚¡ã‚¤ãƒ«ã‚°ãƒ«ãƒ¼ãƒ—ã®ãƒ«ãƒ¼ãƒˆåã€ã‚’æŠ½å‡º (BXXXXXX)
                # INPUT_CSV_FILES_DIR ã«ã¯ B*020.csv ã®ã¿ãŒå­˜åœ¨ã™ã‚‹ã¨ä»®å®š
                # B*020.csv ã®ãƒ‘ã‚¿ãƒ¼ãƒ³ã«åˆè‡´ã™ã‚‹ã‚‚ã®ã®ã¿ã‚’å‡¦ç†
                match = re.match(r'^(B\d{6})_.*\.jpg_020\.csv$', filename, re.IGNORECASE) 
                
                if match: # ãƒ‘ã‚¿ãƒ¼ãƒ³ã«åˆè‡´ã—ãŸå ´åˆã®ã¿å‡¦ç†
                    all_target_file_groups_root.add(match.group(1)) 
                else:
                    print(f"  â„¹ï¸ ocr_result_idç”Ÿæˆå¯¾è±¡å¤–ã®ãƒ•ã‚¡ã‚¤ãƒ«å: {filename} (ãƒ‘ã‚¿ãƒ¼ãƒ³ä¸ä¸€è‡´)ã€‚")
                    
    sorted_file_groups_root = sorted(list(all_target_file_groups_root)) 
    
    for group_root_name in sorted_file_groups_root:
        get_ocr_result_id_for_group(group_root_name) 
    
    print("--- ocr_result_id ãƒãƒƒãƒ”ãƒ³ã‚°äº‹å‰ç”Ÿæˆå®Œäº† ---")
    print(f"ç”Ÿæˆã•ã‚ŒãŸ ocr_id_mapping (æœ€åˆã®5ã¤): {list(ocr_id_mapping.items())[:5]}...")

    # ç”Ÿæˆã—ãŸ ocr_id_mapping ã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
    ocr_id_map_filepath = os.path.join(MASTER_DATA_DIR, 'ocr_id_mapping.json') 
    try:
        with open(ocr_id_map_filepath, 'w', encoding='utf-8') as f:
            json.dump(ocr_id_mapping, f, ensure_ascii=False, indent=4)
        print(f"  âœ… ocr_id_mapping ã‚’ {ocr_id_map_filepath} ã«ä¿å­˜ã—ã¾ã—ãŸã€‚")
    except Exception as e:
            print(f"âŒ ã‚¨ãƒ©ãƒ¼: ocr_id_mapping ã®ä¿å­˜ã«å¤±æ•—ã—ã¾ã—ãŸã€‚ã‚¨ãƒ©ãƒ¼: {e}")

    # ãƒ¡ã‚¤ãƒ³ã®ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†ãƒ«ãƒ¼ãƒ—
    for root, dirs, files in os.walk(INPUT_CSV_FILES_DIR): 
        for filename in files:
            if filename.lower().endswith('.csv') and not filename.lower().endswith('_processed.csv'): 
                input_filepath = os.path.join(root, filename)
                print(f"\n--- å‡¦ç†å¯¾è±¡ãƒ•ã‚¡ã‚¤ãƒ«: {input_filepath} ---")

                current_file_group_root_name = None
                # ãƒ•ã‚¡ã‚¤ãƒ«åã‹ã‚‰ã€Œãƒ•ã‚¡ã‚¤ãƒ«ã‚°ãƒ«ãƒ¼ãƒ—ã®ãƒ«ãƒ¼ãƒˆåã€ã‚’æŠ½å‡º (BXXXXXX)
                # INPUT_CSV_FILES_DIR ã«ã¯ B*020.csv ã®ã¿ãŒå­˜åœ¨ã™ã‚‹ã¨ä»®å®š
                # B*020.csv ã®ãƒ‘ã‚¿ãƒ¼ãƒ³ã«åˆè‡´ã™ã‚‹ã‚‚ã®ã®ã¿ã‚’å‡¦ç†
                match = re.match(r'^(B\d{6})_.*\.jpg_020\.csv$', filename, re.IGNORECASE)
                if match:
                    current_file_group_root_name = match.group(1) 
                
                if current_file_group_root_name is None:
                    print(f"  âš ï¸ è­¦å‘Š: ãƒ•ã‚¡ã‚¤ãƒ« {filename} ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚°ãƒ«ãƒ¼ãƒ—ã®ãƒ«ãƒ¼ãƒˆåã‚’ç‰¹å®šã§ãã¾ã›ã‚“ã§ã—ãŸã€‚ã“ã®ãƒ•ã‚¡ã‚¤ãƒ«ã¯ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
                    continue 

                process_universal_csv(input_filepath, PROCESSED_OUTPUT_BASE_DIR, INPUT_CSV_FILES_DIR, 
                                    maker_master_df, ocr_id_mapping, current_file_group_root_name, 
                                    FINAL_POSTGRE_COLUMNS, NO_HEADER_MAPPING_DICT, HAND_BILL_MAPPING_DICT, 
                                    FINANCIAL_STATEMENT_MAPPING_DICT, LOAN_DETAILS_MAPPING_DICT) 

    print(f"\nğŸ‰ å…¨ã¦ã®ãƒ•ã‚¡ã‚¤ãƒ«ã®åŠ å·¥å‡¦ç†ãŒå®Œäº†ã—ã¾ã—ãŸï¼ ({datetime.now()}) ğŸ‰")
