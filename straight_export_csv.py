import pandas as pd
import os
import re
from datetime import datetime
import random 

# --- è¨­å®šé …ç›®ï¼ˆã“ã“ã ã‘ã€ãã¾ã¡ã‚ƒã‚“ã®ç’°å¢ƒã«åˆã‚ã›ã¦ä¿®æ­£ã—ã¦ã­ï¼ï¼‰ ---
# AIReadãŒå‡ºåŠ›ã—ãŸCSVãƒ•ã‚¡ã‚¤ãƒ«ãŒä¿å­˜ã•ã‚Œã¦ã„ã‚‹ãƒ«ãƒ¼ãƒˆãƒ•ã‚©ãƒ«ãƒ€ (æ± ä¸Š, ä¸­å³¶, å”æœ¨ãƒ•ã‚©ãƒ«ãƒ€ãŒã‚ã‚‹å ´æ‰€)
# ä¾‹: r'G:\å…±æœ‰ãƒ‰ãƒ©ã‚¤ãƒ–\VLM-OCR\20_æ•™å¸«ãƒ‡ãƒ¼ã‚¿\30_output_csv'
INPUT_BASE_DIR = r'G:\å…±æœ‰ãƒ‰ãƒ©ã‚¤ãƒ–\VLM-OCR\20_æ•™å¸«ãƒ‡ãƒ¼ã‚¿\30_output_csv' 

# åŠ å·¥å¾Œã®CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¿å­˜ã™ã‚‹ãƒ«ãƒ¼ãƒˆãƒ•ã‚©ãƒ«ãƒ€
# ä¾‹: r'C:\Users\User26\yoko\dev\csvRead\output'
OUTPUT_BASE_DIR = r'C:\Users\User26\yoko\dev\csvRead\output' 

# å…ƒã®CSVãƒ•ã‚¡ã‚¤ãƒ«ã®æœ€å¤§ã‚«ãƒ©ãƒ æ•°ã«åˆã‚ã›ã¦èª¿æ•´ã—ã¦ã­ã€‚
# ä¾‹ãˆã°ã€ä¸€ç•ªæ¨ªã«é•·ã„CSVãŒ20ã‚«ãƒ©ãƒ ã‚ã‚‹ãªã‚‰ 20 ã‚’è¨­å®šã™ã‚‹ã€‚
# ã“ã‚Œã‚ˆã‚Šå°‘ãªã„ã¨ãƒ‡ãƒ¼ã‚¿ãŒæ¬ ã‘ã‚‹å¯èƒ½æ€§ãŒã‚ã‚‹ã‚ˆã€‚
MAX_GENERIC_COLUMNS = 20 # æ±ç”¨ã‚«ãƒ©ãƒ ã®æ•°ã‚’20ã«å¢—ã‚„ã—ãŸã‚ˆã€‚å¿…è¦ãªã‚‰ã•ã‚‰ã«å¢—ã‚„ã—ã¦ã­ï¼

# --- é–¢æ•°å®šç¾©ï¼ˆã“ã®éƒ¨åˆ†ã¯å¤‰æ›´ã—ãªã„ã§ã­ï¼ï¼‰ ---
def process_any_csv(input_filepath, output_base_dir, input_base_dir, max_generic_cols, maker_master_df, jgroupid_master_df):
    """
    ä»»æ„ã®å½¢å¼ã®CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿ã€æ±ç”¨çš„ãªã‚«ãƒ©ãƒ å½¢å¼ã§å‡ºåŠ›ã™ã‚‹é–¢æ•°
    """
    try:
        # CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚€
        # ã€Œãµã¤ã†ã®UTF-8ã€ã¨ã®ã“ã¨ãªã®ã§ã€encoding='utf-8' ã‚’ç›´æ¥æŒ‡å®šã™ã‚‹ã‚ˆã€‚
        # header=None ã§ãƒ˜ãƒƒãƒ€ãƒ¼ã‚’èª­ã¿è¾¼ã¾ãšã€ã™ã¹ã¦ã®è¡Œã‚’ãƒ‡ãƒ¼ã‚¿ã¨ã—ã¦æ‰±ã†ã€‚
        df_original = pd.read_csv(input_filepath, encoding='utf-8', header=None)
        print(f"  ãƒ•ã‚¡ã‚¤ãƒ« {os.path.basename(input_filepath)} ã‚’ UTF-8 ã§èª­ã¿è¾¼ã¿æˆåŠŸã€‚")
            
    except UnicodeDecodeError as ude:
        # UTF-8ã§èª­ã¿è¾¼ã‚ãªã‹ã£ãŸå ´åˆã®å‡¦ç†ã€‚
        # ã‚‚ã—UTF-8-BOMä»˜ãã®CSVãŒæ··ã˜ã£ã¦ã„ãŸå ´åˆã«å‚™ãˆã¦ã€ä¸€åº¦ 'utf-8-sig' ã‚‚è©¦ã—ã¦ã¿ã‚‹ã‚ˆã€‚
        try:
            df_original = pd.read_csv(input_filepath, encoding='utf-8-sig', header=None)
            print(f"  ãƒ•ã‚¡ã‚¤ãƒ« {os.path.basename(input_filepath)} ã‚’ UTF-8-BOM ã§èª­ã¿è¾¼ã¿æˆåŠŸã€‚")
        except Exception as e_sig:
            # ã©ã¡ã‚‰ã§ã‚‚èª­ã¿è¾¼ã‚ãªã‹ã£ãŸã‚‰ã‚¨ãƒ©ãƒ¼ã¨ã—ã¦å ±å‘Š
            print(f"âŒ ã‚¨ãƒ©ãƒ¼ç™ºç”Ÿï¼ˆ{input_filepath}ï¼‰: UTF-8ã§ã‚‚UTF-8-BOMã§ã‚‚èª­ã¿è¾¼ã‚ã¾ã›ã‚“ã§ã—ãŸã€‚ã‚¨ãƒ©ãƒ¼: {ude}, {e_sig}")
            import traceback
            traceback.print_exc()
            return # ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ãŸå ´åˆã¯ã“ã®ãƒ•ã‚¡ã‚¤ãƒ«ã®å‡¦ç†ã‚’ä¸­æ–­ã—ã¦æ¬¡ã¸
    except Exception as e:
        # ãã®ä»–ã®äºˆæœŸã›ã¬ã‚¨ãƒ©ãƒ¼ã®å ´åˆ
        print(f"âŒ ã‚¨ãƒ©ãƒ¼ç™ºç”Ÿï¼ˆ{input_filepath}ï¼‰: CSVèª­ã¿è¾¼ã¿ä¸­ã«äºˆæœŸã›ã¬å•é¡ŒãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚ã‚¨ãƒ©ãƒ¼: {e}")
        import traceback
        traceback.print_exc()
        return # ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ãŸå ´åˆã¯ã“ã®ãƒ•ã‚¡ã‚¤ãƒ«ã®å‡¦ç†ã‚’ä¸­æ–­ã—ã¦æ¬¡ã¸

    # --- ãƒ‡ãƒ¼ã‚¿åŠ å·¥å‡¦ç†ï¼ˆã“ã“ã‹ã‚‰ä¸‹ã¯å¤‰æ›´ã—ãªã„ã§ã­ï¼‰ ---
    df_processed = pd.DataFrame() # é–¢æ•°ã®å…ˆé ­ã«ç§»å‹•æ¸ˆã¿

    # ãƒ•ã‚¡ã‚¤ãƒ«ãŒæ‰‹å½¢æƒ…å ±ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã‹ã©ã†ã‹ã‚’åˆ¤æ–­ã™ã‚‹ãƒ•ãƒ©ã‚°
    is_bill_format = False
    
    # å…ƒã®CSVã®ãƒ˜ãƒƒãƒ€ãƒ¼ï¼ˆã¾ãŸã¯æœ€åˆã®è¡Œã®ãƒ‡ãƒ¼ã‚¿ï¼‰ã‚’è¦‹ã¦åˆ¤æ–­
    if not df_original.empty:
        # æœ€åˆã®è¡Œã®ãƒ‡ãƒ¼ã‚¿ã‚’æ–‡å­—åˆ—ã¨ã—ã¦çµåˆã—ã¦ç¢ºèª (ãƒ‡ãƒãƒƒã‚°ç”¨)
        first_row_content = " ".join(df_original.iloc[0, :].astype(str).values)
        print(f"  ãƒ‡ãƒãƒƒã‚°ï¼ˆ{os.path.basename(input_filepath)}ï¼‰: æœ€åˆã®è¡Œã®å†…å®¹: '{first_row_content}'")
        
        # ã€ŒæŒ¯å‡ºäººã€ãŒæœ€åˆã®è¡Œã«å«ã¾ã‚Œã¦ã„ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
        # ã“ã®æ¡ä»¶ã§æ‰‹å½¢æƒ…å ±ã‹ã©ã†ã‹ã‚’åˆ¤æ–­ã™ã‚‹
        if 'æŒ¯å‡ºäºº' in first_row_content:
             is_bill_format = True
        
        print(f"  ãƒ‡ãƒãƒƒã‚°ï¼ˆ{os.path.basename(input_filepath)}ï¼‰: is_bill_format = {is_bill_format}")

    if is_bill_format:
        # --- æ‰‹å½¢æƒ…å ±ã®åŠ å·¥ãƒ­ã‚¸ãƒƒã‚¯ ---
        # df_airead ã«ç›¸å½“ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ãŒ df_original ã«å…¥ã£ã¦ã„ã‚‹

        # ocr_result_id ã‚’ç”Ÿæˆ (ãƒ•ã‚¡ã‚¤ãƒ«ã”ã¨ã«ä¸€æ„)
        timestamp = datetime.now().strftime('%Y%m%d%H%M%S')
        random_suffix = str(random.randint(0, 9999)).zfill(4) 
        ocr_result_id = f"{timestamp}{random_suffix}"

        # page_no: ãƒ•ã‚¡ã‚¤ãƒ«åã‹ã‚‰ãƒšãƒ¼ã‚¸ç•ªå·ã‚’æŠ½å‡º (ä¾‹: B000039_2.jpg_030.csv ã‹ã‚‰ 2 ã‚’æŠ½å‡º)
        page_no_match = re.search(r'_(?P<page_num>\d+)(?:\.jpg_(\d+))?\.csv$', os.path.basename(input_filepath))
        if page_no_match:
            page_no = int(page_no_match.group('page_num'))
        else:
            page_no = 1 # è¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤

        # jgroupid_string: jgroupid_masterã‹ã‚‰ãƒ©ãƒ³ãƒ€ãƒ ã«1ã¤é¸æŠ
        jgroupid_string = "000" # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤
        if not jgroupid_master_df.empty and 'jgroupid' in jgroupid_master_df.columns:
            jgroupid_string = random.choice(jgroupid_master_df['jgroupid'].tolist())

        # cif_number: ãƒ©ãƒ³ãƒ€ãƒ ãªæ•°å­—åˆ—ï¼ˆ6æ¡ã®ä¾‹ï¼‰
        cif_number = str(random.randint(100000, 999999))

        settlement_at = datetime.now().strftime('%Y%m') # YYYYMMå½¢å¼

        # å…±é€šãƒ‡ãƒ¼ã‚¿ã‚’DataFrameã«è¨­å®š (ãƒ˜ãƒƒãƒ€ãƒ¼è¡Œã‚’é™¤ããƒ‡ãƒ¼ã‚¿è¡Œæ•°åˆ†)
        df_data_rows = df_original.iloc[1:].copy() # ãƒ˜ãƒƒãƒ€ãƒ¼è¡Œã‚’ã‚¹ã‚­ãƒƒãƒ—

        # id: ãƒ•ã‚¡ã‚¤ãƒ«å†…ã§ã‚«ã‚¦ãƒ³ãƒˆã‚¢ãƒƒãƒ— (å„è¡Œã«ãƒ¦ãƒ‹ãƒ¼ã‚¯ãªID)
        df_processed['id'] = range(1, len(df_data_rows) + 1)
        
        # df_data_rowsã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’ãƒªã‚»ãƒƒãƒˆã—ã¦ã€å…ƒã®åˆ—ã‚’çµåˆã—ã‚„ã™ãã™ã‚‹
        df_data_rows.reset_index(drop=True, inplace=True)
        
        # å…ƒã®CSVã®ãƒ‡ãƒ¼ã‚¿åˆ—ã‚’ df_processed ã«çµåˆ
        # ã“ã“ã§ã€df_originalã®åˆ—ç•ªå·ãŒãã®ã¾ã¾ã‚«ãƒ©ãƒ åã¨ã—ã¦ä½¿ã‚ã‚Œã‚‹
        for col_idx in df_data_rows.columns:
            df_processed[col_idx] = df_data_rows[col_idx]

        # ã‚«ãƒ©ãƒ åã‚’å…ƒã®CSVã®ãƒ˜ãƒƒãƒ€ãƒ¼åã«ãƒãƒƒãƒ”ãƒ³ã‚°
        # ä¾‹ï¼šdf_processed.rename(columns={0: 'æŒ¯å‡ºäºº', 1: 'æŒ¯å‡ºå¹´æœˆæ—¥', ...}, inplace=True)
        # å®Ÿéš›ã«æä¾›ã•ã‚ŒãŸæ‰‹å½¢ãƒ‡ãƒ¼ã‚¿ã®ã‚«ãƒ©ãƒ é †ç•ªã¨åå‰ã‚’åŸºã«ãƒãƒƒãƒ”ãƒ³ã‚°
        temp_col_map = {
            0: 'æŒ¯å‡ºäºº', 1: 'æŒ¯å‡ºå¹´æœˆæ—¥', 2: 'æ”¯æ‰•æœŸæ—¥', 3: 'æ”¯æ‰•éŠ€è¡Œåç§°',
            4: 'æ”¯æ‰•éŠ€è¡Œæ”¯åº—å', 5: 'é‡‘é¡', 6: 'å‰²å¼•éŠ€è¡ŒååŠã³æ”¯åº—åç­‰', 7: 'æ‘˜è¦'
        }
        # å¿…è¦ãªã‚«ãƒ©ãƒ ã ã‘ãƒªãƒãƒ¼ãƒ ã—ã€è¶³ã‚Šãªã„å ´åˆã¯å‡¦ç†ã—ãªã„
        rename_cols = {c_idx: new_name for c_idx, new_name in temp_col_map.items() if c_idx in df_processed.columns}
        df_processed.rename(columns=rename_cols, inplace=True)

        # maker_name_original ã¨ maker_name ã‚’è¨­å®š
        if 'æŒ¯å‡ºäºº' in df_processed.columns:
            df_processed['maker_name_original'] = df_processed['æŒ¯å‡ºäºº'].fillna('').astype(str)
            df_processed['maker_name'] = df_processed['æŒ¯å‡ºäºº'].fillna('').astype(str)
        else:
            df_processed['maker_name_original'] = ''
            df_processed['maker_name'] = ''

        # maker_com_code: maker_master_dfã‹ã‚‰VLOOKUPã®ã‚ˆã†ã«çµåˆ
        if 'æŒ¯å‡ºäºº' in df_processed.columns and not maker_master_df.empty:
            df_processed = pd.merge(df_processed, maker_master_df[['ä¼šç¤¾å', 'ä¼šç¤¾ã‚³ãƒ¼ãƒ‰']],
                                    left_on='æŒ¯å‡ºäºº', right_on='ä¼šç¤¾å', how='left', suffixes=('', '_master'))
            df_processed['maker_com_code'] = df_processed['ä¼šç¤¾ã‚³ãƒ¼ãƒ‰'].fillna('').astype(str)
            # çµåˆã«ä½¿ã£ãŸä¸€æ™‚çš„ãªã‚«ãƒ©ãƒ ã‚’å‰Šé™¤
            df_processed = df_processed.drop(columns=['ä¼šç¤¾å', 'ä¼šç¤¾ã‚³ãƒ¼ãƒ‰'], errors='ignore')
        else:
            df_processed['maker_com_code'] = '' # æŒ¯å‡ºäººãŒãªã„ã‹ãƒã‚¹ã‚¿ãŒãªã„å ´åˆã¯ç©º

        # æ—¥ä»˜ã¨é‡‘é¡ã€éŠ€è¡Œæƒ…å ±ã®ãƒãƒƒãƒ”ãƒ³ã‚°
        mapping_rules = {
            'æŒ¯å‡ºå¹´æœˆæ—¥': ['issue_date_rightside_date', 'issue_date'],
            'æ”¯æ‰•æœŸæ—¥': ['due_date_rightside_date', 'due_date'],
            'é‡‘é¡': ['balance_rightside', 'balance'],
            'æ”¯æ‰•éŠ€è¡Œåç§°': ['payment_bank_name_rightside', 'payment_bank_name'],
            'æ”¯æ‰•éŠ€è¡Œæ”¯åº—å': ['payment_bank_branch_name_rightside', 'payment_bank_branch_name']
        }

        for col_orig, target_cols in mapping_rules.items():
            if col_orig in df_processed.columns:
                for target_col in target_cols:
                    df_processed[target_col] = df_processed[col_orig].fillna('').astype(str)
            else:
                for target_col in target_cols:
                    df_processed[target_col] = ''

        # å‰²å¼•éŠ€è¡ŒååŠã³æ”¯åº—åç­‰ / æ‘˜è¦ ã®ãƒãƒƒãƒ”ãƒ³ã‚°
        if 'å‰²å¼•éŠ€è¡ŒååŠã³æ”¯åº—åç­‰' in df_processed.columns:
            df_processed['description_rightside'] = df_processed['å‰²å¼•éŠ€è¡ŒååŠã³æ”¯åº—åç­‰'].fillna('').astype(str)
        else:
            df_processed['description_rightside'] = ''

        if 'æ‘˜è¦' in df_processed.columns:
            df_processed['description'] = df_processed['æ‘˜è¦'].fillna('').astype(str)
        else:
            df_processed['description'] = ''
        
        # ocr_result_id, page_no, jgroupid_string, cif_number, settlement_at ã‚’çµåˆã™ã‚‹
        # ã“ã®æ™‚ç‚¹ã§df_processedã«ã¯idã¨å…ƒã®ãƒ‡ãƒ¼ã‚¿ãŒçµåˆã•ã‚Œã¦ã„ã‚‹çŠ¶æ…‹
        # ã“ã‚Œã‚‰ã®å€¤ã‚’å„è¡Œã«è¨­å®šã™ã‚‹ãŸã‚ã«ã€ã‚‚ã†ä¸€åº¦ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’çµåˆã™ã‚‹
        fixed_common_data = {
            'ocr_result_id': ocr_result_id,
            'page_no': page_no,
            'jgroupid_string': jgroupid_string,
            'cif_number': cif_number,
            'settlement_at': settlement_at
        }
        # å„è¡Œã«åŒã˜å€¤ã‚’è¨­å®š
        for k, v in fixed_common_data.items():
            df_processed[k] = v

        # æœ€çµ‚çš„ãªåˆ—ã®é †åºã‚’ç›®æ¨™ã®å½¢å¼ã«åˆã‚ã›ã‚‹
        output_columns_bill = [
            'ocr_result_id', 'page_no', 'id', 'jgroupid_string', 'cif_number',
            'settlement_at', 'maker_name_original', 'maker_name', 'maker_com_code',
            'issue_date_rightside_date', 'issue_date',
            'due_date_rightside_date', 'due_date',
            'balance_rightside', 'balance',
            'payment_bank_name_rightside', 'payment_bank_name',
            'payment_bank_branch_name_rightside', 'payment_bank_branch_name',
            'description_rightside', 'description'
        ]
        # å­˜åœ¨ã—ãªã„ã‚«ãƒ©ãƒ ã¯ç„¡è¦–ã—ã¦ã€å­˜åœ¨ã™ã‚‹ã‚«ãƒ©ãƒ ã®ã¿é †åºã‚’é©ç”¨
        # ã¾ãŸã€å…ƒã®æ—¥æœ¬èªã‚«ãƒ©ãƒ ï¼ˆæŒ¯å‡ºäººãªã©ï¼‰ã¯æœ€çµ‚å‡ºåŠ›ã‹ã‚‰é™¤å¤–ã™ã‚‹
        df_processed = df_processed.reindex(columns=[col for col in output_columns_bill if col in df_processed.columns])


    else:
        # --- ãã®ä»–ã®å½¢å¼ã®åŠ å·¥ãƒ­ã‚¸ãƒƒã‚¯ï¼ˆä»¥å‰ã®ã€Œã‚¹ãƒˆãƒ¬ãƒ¼ãƒˆã«åãå‡ºã™ã€å‡¦ç†ï¼‰ ---
        # df_original ã‚’ãã®ã¾ã¾æ´»ç”¨ã—ã€æ±ç”¨ã‚«ãƒ©ãƒ ã«å‰²ã‚Šå½“ã¦ã‚‹

        # file_path: å…ƒãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ•ãƒ«ãƒ‘ã‚¹
        df_processed['file_path'] = input_filepath
        # original_file_name: å…ƒãƒ•ã‚¡ã‚¤ãƒ«å
        df_processed['original_file_name'] = os.path.basename(input_filepath)

        # page_index: ãƒ•ã‚¡ã‚¤ãƒ«åã‹ã‚‰ _1, _2 ãªã©ã‚’æŠ½å‡º
        page_idx_match = re.search(r'_(?P<page_num>\d+)(?:\.jpg_(\d+))?\.csv$', os.path.basename(input_filepath))
        if page_idx_match:
            df_processed['page_index'] = int(page_idx_match.group('page_num'))
        else:
            first_num_match = re.search(r'(\d+)', os.path.basename(input_filepath))
            if first_num_match:
                df_processed['page_index'] = int(first_num_match.group(1))
            else:
                df_processed['page_index'] = 0

        # row_number: å…ƒCSVå†…ã§ã®è¡Œç•ªå· (0ã‹ã‚‰å§‹ã¾ã‚‹DataFrameã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹)
        df_processed['row_number'] = df_original.index

        # original_header_text: æœ€åˆã®ã‚«ãƒ©ãƒ ã®å€¤ã‚’æ ¼ç´
        df_processed['original_header_text'] = df_original.iloc[:, 0].fillna('').astype(str)

        # æ±ç”¨ã‚«ãƒ©ãƒ ã«ãƒ‡ãƒ¼ã‚¿ã‚’æ ¼ç´
        for i in range(max_generic_cols):
            col_name = f'column_{i+1}_value'
            if i < df_original.shape[1]:
                df_processed[col_name] = df_original.iloc[:, i].fillna('').astype(str)
            else:
                df_processed[col_name] = ''
        
        # å‡ºåŠ›ã‚«ãƒ©ãƒ ã®é †åºã‚’å®šç¾© (æ±ç”¨å½¢å¼ç”¨)
        # å…¨ã¦ã®æ±ç”¨ã‚«ãƒ©ãƒ ãŒå‡ºåŠ›ã•ã‚Œã‚‹ã‚ˆã†ã«ã™ã‚‹
        generic_output_columns = ['file_path', 'original_file_name', 'page_index', 'row_number', 'original_header_text'] + \
                                 [f'column_{i+1}_value' for i in range(max_generic_cols)]
        df_processed = df_processed.reindex(columns=generic_output_columns)

    # --- ä¿å­˜å‡¦ç†ï¼ˆã“ã®éƒ¨åˆ†ã¯å¤‰æ›´ãªã—ï¼‰ ---
    # å‡ºåŠ›å…ˆã®ã‚µãƒ–ãƒ•ã‚©ãƒ«ãƒ€ã‚’å…ƒã®ãƒ•ã‚©ãƒ«ãƒ€æ§‹é€ ã«åˆã‚ã›ã¦ä½œæˆ
    relative_path_to_file = os.path.relpath(input_filepath, input_base_dir)
    relative_dir_to_file = os.path.dirname(relative_path_to_file)
    output_sub_dir = os.path.join(output_base_dir, relative_dir_to_file)
    os.makedirs(output_sub_dir, exist_ok=True)

    # åŠ å·¥å¾Œã®CSVã‚’ä¿å­˜
    output_filename = os.path.basename(input_filepath).replace('.csv', '_processed.csv')
    output_filepath = os.path.join(output_sub_dir, output_filename)
    df_processed.to_csv(output_filepath, index=False, encoding='utf-8-sig')

    print(f"âœ… åŠ å·¥å®Œäº†: {input_filepath} -> {output_filepath}")

# --- ãƒ¡ã‚¤ãƒ³å‡¦ç†ï¼ˆã“ã®éƒ¨åˆ†ã‚‚å¤‰æ›´ã—ãªã„ã§ã­ï¼ï¼‰ ---
if __name__ == "__main__":
    print(f"--- å‡¦ç†é–‹å§‹: {datetime.now()} ---")

    # å‡ºåŠ›ãƒ•ã‚©ãƒ«ãƒ€ãŒãªã‘ã‚Œã°ä½œæˆ
    os.makedirs(OUTPUT_BASE_DIR, exist_ok=True) 

    # ãƒã‚¹ã‚¿ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
    MASTER_DATA_DIR = r'C:\AIRead\master_data' # <--- ã“ã“ã¯ãã¾ã¡ã‚ƒã‚“ãŒãƒã‚¹ã‚¿ã‚’ä¿å­˜ã—ãŸãƒ‘ã‚¹ã«åˆã‚ã›ã¦ã­ï¼

    # maker_master.csv ã‚’èª­ã¿è¾¼ã‚€
    maker_master_filepath = os.path.join(MASTER_DATA_DIR, 'maker_master.csv')
    maker_master_df = pd.DataFrame() 
    if os.path.exists(maker_master_filepath):
        try:
            maker_master_df = pd.read_csv(maker_master_filepath, encoding='shift_jis') 
        except Exception as e:
            print(f"âŒ ã‚¨ãƒ©ãƒ¼: maker_master.csv ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸã€‚ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚ã‚¨ãƒ©ãƒ¼: {e}")
            maker_master_df = pd.DataFrame({'ä¼šç¤¾å': [], 'ä¼šç¤¾ã‚³ãƒ¼ãƒ‰': []}) # ç©ºã®DataFrameã§ç¶™ç¶š
    else:
        print(f"âš ï¸ è­¦å‘Š: maker_master.csv ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚ãƒ‘ã‚¹ã‚’ç¢ºèªã—ã¦ãã ã•ã„: {maker_master_filepath}")
        maker_master_data = {
            'ä¼šç¤¾å': ['(æ ª)åŒæ–‡ç¤¾å°åˆ·', '(æ ª)å¤ªå¹³å°åˆ·ç¤¾', '(æ ª)ãƒªãƒ¼ãƒ–ãƒ«ãƒ†ãƒƒã‚¯', 'æ—¥æœ¬ãƒã‚¤ã‚³ãƒ (æ ª)', '(æ ª)æ–°å¯¿å ‚', 'æ‰‹æŒæ‰‹å½¢è¨ˆ', 'å‰²å¼•æ‰‹å½¢è¨ˆ', '(æ ª)ã‚·ãƒ¼ãƒ•ã‚©ãƒ¼ã‚¹'],
            'ä¼šç¤¾ã‚³ãƒ¼ãƒ‰': ['4380946945', '9138429316', '2578916640', '5408006886', '0668992415', '9443492307', '4417864013', '7398659210']
        }
        maker_master_df = pd.DataFrame(maker_master_data)

    # jgroupid_master.csv ã‚’èª­ã¿è¾¼ã‚€
    jgroupid_master_filepath = os.path.join(MASTER_DATA_DIR, 'jgroupid_master.csv')
    jgroupid_master_df = pd.DataFrame() 
    if os.path.exists(jgroupid_master_filepath): # <<-- ã“ã“ã‚’ jgroupid_master_filepath ã«ä¿®æ­£æ¸ˆã¿ï¼
        try:
            jgroupid_master_df = pd.read_csv(jgroupid_master_filepath, encoding='shift_jis')
        except Exception as e:
            print(f"âŒ ã‚¨ãƒ©ãƒ¼: jgroupid_master.csv ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸã€‚ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚ã‚¨ãƒ©ãƒ¼: {e}")
            jgroupid_master_df = pd.DataFrame({'jgroupid': []}) # ç©ºã®DataFrameã§ç¶™ç¶š
    else:
        print(f"âš ï¸ è­¦å‘Š: jgroupid_master.csv ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚ãƒ‘ã‚¹ã‚’ç¢ºèªã—ã¦ãã ã•ã„: {jgroupid_master_filepath}")
        jgroupids = [f"{i:03d}" for i in range(1, 94)] 
        jgroupid_master_df = pd.DataFrame({'jgroupid': jgroupids})


    # INPUT_BASE_DIRå†…ã®å…¨ã¦ã®CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‡¦ç†
    for root, dirs, files in os.walk(INPUT_BASE_DIR):
        for filename in files:
            if filename.lower().endswith('.csv'): 
                input_filepath = os.path.join(root, filename)
                # ãƒã‚¹ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’ process_any_csv é–¢æ•°ã«æ¸¡ã™
                process_any_csv(input_filepath, OUTPUT_BASE_DIR, INPUT_BASE_DIR, MAX_GENERIC_COLUMNS, maker_master_df, jgroupid_master_df)

    print(f"\nğŸ‰ å…¨ã¦ã®ãƒ•ã‚¡ã‚¤ãƒ«ã®åŠ å·¥å‡¦ç†ãŒå®Œäº†ã—ã¾ã—ãŸï¼ ({datetime.now()}) ğŸ‰")
    